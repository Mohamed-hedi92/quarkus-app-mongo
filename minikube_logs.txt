
==> Audit <==
|---------|------|----------|-----------|---------|----------------------|----------------------|
| Command | Args | Profile  |   User    | Version |      Start Time      |       End Time       |
|---------|------|----------|-----------|---------|----------------------|----------------------|
| start   |      | minikube | ISOAD\bou | v1.33.1 | 20 Jun 24 10:14 CEST | 20 Jun 24 10:18 CEST |
| ip      |      | minikube | ISOAD\bou | v1.33.1 | 20 Jun 24 11:09 CEST | 20 Jun 24 11:09 CEST |
| ip      |      | minikube | ISOAD\bou | v1.33.1 | 20 Jun 24 13:10 CEST | 20 Jun 24 13:10 CEST |
| ip      |      | minikube | ISOAD\bou | v1.33.1 | 20 Jun 24 13:26 CEST | 20 Jun 24 13:26 CEST |
| stop    |      | minikube | ISOAD\bou | v1.33.1 | 20 Jun 24 13:32 CEST | 20 Jun 24 13:32 CEST |
| start   |      | minikube | ISOAD\bou | v1.33.1 | 20 Jun 24 13:32 CEST | 20 Jun 24 13:32 CEST |
| ip      |      | minikube | ISOAD\bou | v1.33.1 | 20 Jun 24 13:37 CEST | 20 Jun 24 13:37 CEST |
| ip      |      | minikube | ISOAD\bou | v1.33.1 | 20 Jun 24 13:45 CEST | 20 Jun 24 13:45 CEST |
| ssh     |      | minikube | ISOAD\bou | v1.33.1 | 20 Jun 24 13:49 CEST |                      |
| ip      |      | minikube | ISOAD\bou | v1.33.1 | 20 Jun 24 13:58 CEST | 20 Jun 24 13:58 CEST |
| start   |      | minikube | ISOAD\bou | v1.33.1 | 20 Jun 24 14:36 CEST | 20 Jun 24 14:36 CEST |
| start   |      | minikube | ISOAD\bou | v1.33.1 | 21 Jun 24 09:12 CEST | 21 Jun 24 09:12 CEST |
| ip      |      | minikube | ISOAD\bou | v1.33.1 | 21 Jun 24 11:09 CEST | 21 Jun 24 11:09 CEST |
| ip      |      | minikube | ISOAD\bou | v1.33.1 | 21 Jun 24 11:34 CEST | 21 Jun 24 11:34 CEST |
| stop    |      | minikube | ISOAD\bou | v1.33.1 | 21 Jun 24 11:38 CEST | 21 Jun 24 11:39 CEST |
| start   |      | minikube | ISOAD\bou | v1.33.1 | 21 Jun 24 11:39 CEST | 21 Jun 24 11:39 CEST |
|---------|------|----------|-----------|---------|----------------------|----------------------|


==> Letzter Start <==
Log file created at: 2024/06/21 11:39:26
Running on machine: NB-27277
Binary: Built with gc go1.22.1 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0621 11:39:26.143034   16580 out.go:291] Setting OutFile to fd 84 ...
I0621 11:39:26.143623   16580 out.go:338] TERM=,COLORTERM=, which probably does not support color
I0621 11:39:26.143623   16580 out.go:304] Setting ErrFile to fd 88...
I0621 11:39:26.143623   16580 out.go:338] TERM=,COLORTERM=, which probably does not support color
W0621 11:39:26.155673   16580 root.go:314] Error reading config file at C:\Users\bou\.minikube\config\config.json: open C:\Users\bou\.minikube\config\config.json: Das System kann die angegebene Datei nicht finden.
I0621 11:39:26.170280   16580 out.go:298] Setting JSON to false
I0621 11:39:26.174123   16580 start.go:129] hostinfo: {"hostname":"NB-27277","uptime":13658,"bootTime":1718949107,"procs":330,"os":"windows","platform":"Microsoft Windows 10 Enterprise","platformFamily":"Standalone Workstation","platformVersion":"10.0.19045.3930 Build 19045.3930","kernelVersion":"10.0.19045.3930 Build 19045.3930","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"9158d5ad-cbc4-4af5-a9f2-239e62a63891"}
W0621 11:39:26.174123   16580 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0621 11:39:26.175158   16580 out.go:177] * minikube v1.33.1 auf Microsoft Windows 10 Enterprise 10.0.19045.3930 Build 19045.3930
I0621 11:39:26.175693   16580 notify.go:220] Checking for updates...
I0621 11:39:26.176205   16580 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0621 11:39:26.176205   16580 driver.go:392] Setting default libvirt URI to qemu:///system
I0621 11:39:26.291594   16580 docker.go:122] docker version: linux-25.0.2:Docker Desktop 4.27.1 (136059)
I0621 11:39:26.295846   16580 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0621 11:39:26.492482   16580 info.go:266] docker info: {ID:08b23e16-d3f5-4859-89e7-0bc1bd5c1280 Containers:8 ContainersRunning:3 ContainersPaused:0 ContainersStopped:5 Images:36 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:74 OomKillDisable:true NGoroutines:102 SystemTime:2024-06-21 09:39:26.460546588 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:14 KernelVersion:5.15.153.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:16351850496 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:25.0.2 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.12.1-desktop.4] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.24.3-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container. Vendor:Docker Inc. Version:0.0.22] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.21] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.0.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.3.0]] Warnings:<nil>}}
I0621 11:39:26.493553   16580 out.go:177] * Verwende den Treiber docker basierend auf dem existierenden Profil
I0621 11:39:26.494088   16580 start.go:297] selected driver: docker
I0621 11:39:26.494088   16580 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:7900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\bou:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0621 11:39:26.494088   16580 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0621 11:39:26.501456   16580 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0621 11:39:26.692841   16580 info.go:266] docker info: {ID:08b23e16-d3f5-4859-89e7-0bc1bd5c1280 Containers:8 ContainersRunning:3 ContainersPaused:0 ContainersStopped:5 Images:36 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:74 OomKillDisable:true NGoroutines:102 SystemTime:2024-06-21 09:39:26.660246329 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:14 KernelVersion:5.15.153.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:16351850496 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:25.0.2 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.12.1-desktop.4] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.24.3-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container. Vendor:Docker Inc. Version:0.0.22] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.21] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.0.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.3.0]] Warnings:<nil>}}
I0621 11:39:26.714151   16580 cni.go:84] Creating CNI manager for ""
I0621 11:39:26.714151   16580 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0621 11:39:26.714151   16580 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:7900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\bou:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0621 11:39:26.714669   16580 out.go:177] * Starte "minikube" primary control-plane Node im "minikube" Cluster
I0621 11:39:26.715211   16580 cache.go:121] Beginning downloading kic base image for docker with docker
I0621 11:39:26.715726   16580 out.go:177] * Ziehe Base Image v0.0.44 ...
I0621 11:39:26.716788   16580 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0621 11:39:26.716788   16580 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e in local docker daemon
I0621 11:39:26.716788   16580 preload.go:147] Found local preload: C:\Users\bou\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4
I0621 11:39:26.716788   16580 cache.go:56] Caching tarball of preloaded images
I0621 11:39:26.716788   16580 preload.go:173] Found C:\Users\bou\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0621 11:39:26.716788   16580 cache.go:59] Finished verifying existence of preloaded tar for v1.30.0 on docker
I0621 11:39:26.716788   16580 profile.go:143] Saving config to C:\Users\bou\.minikube\profiles\minikube\config.json ...
I0621 11:39:26.816322   16580 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e in local docker daemon, skipping pull
I0621 11:39:26.816322   16580 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e exists in daemon, skipping load
I0621 11:39:26.816322   16580 cache.go:194] Successfully downloaded all kic artifacts
I0621 11:39:26.816322   16580 start.go:360] acquireMachinesLock for minikube: {Name:mk6025587e0d165728d33365f549e3bef878af15 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0621 11:39:26.816322   16580 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I0621 11:39:26.816322   16580 start.go:96] Skipping create...Using existing machine configuration
I0621 11:39:26.816322   16580 fix.go:54] fixHost starting: 
I0621 11:39:26.824205   16580 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0621 11:39:26.909570   16580 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0621 11:39:26.909570   16580 fix.go:138] unexpected machine state, will restart: <nil>
I0621 11:39:26.910605   16580 out.go:177] * Starte existierenden docker container f√ºr "minikube" ...
I0621 11:39:26.914886   16580 cli_runner.go:164] Run: docker start minikube
I0621 11:39:27.314689   16580 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0621 11:39:27.402131   16580 kic.go:430] container "minikube" state is running.
I0621 11:39:27.406931   16580 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0621 11:39:27.494421   16580 profile.go:143] Saving config to C:\Users\bou\.minikube\profiles\minikube\config.json ...
I0621 11:39:27.495989   16580 machine.go:94] provisionDockerMachine start ...
I0621 11:39:27.499648   16580 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0621 11:39:27.587768   16580 main.go:141] libmachine: Using SSH client type: native
I0621 11:39:27.587768   16580 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x5fa3c0] 0x5fcfa0 <nil>  [] 0s} 127.0.0.1 57276 <nil> <nil>}
I0621 11:39:27.587768   16580 main.go:141] libmachine: About to run SSH command:
hostname
I0621 11:39:27.714973   16580 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0621 11:39:27.714973   16580 ubuntu.go:169] provisioning hostname "minikube"
I0621 11:39:27.719697   16580 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0621 11:39:27.804872   16580 main.go:141] libmachine: Using SSH client type: native
I0621 11:39:27.804872   16580 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x5fa3c0] 0x5fcfa0 <nil>  [] 0s} 127.0.0.1 57276 <nil> <nil>}
I0621 11:39:27.804872   16580 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0621 11:39:27.941051   16580 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0621 11:39:27.945214   16580 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0621 11:39:28.034620   16580 main.go:141] libmachine: Using SSH client type: native
I0621 11:39:28.034620   16580 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x5fa3c0] 0x5fcfa0 <nil>  [] 0s} 127.0.0.1 57276 <nil> <nil>}
I0621 11:39:28.034620   16580 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0621 11:39:28.164264   16580 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0621 11:39:28.164264   16580 ubuntu.go:175] set auth options {CertDir:C:\Users\bou\.minikube CaCertPath:C:\Users\bou\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\bou\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\bou\.minikube\machines\server.pem ServerKeyPath:C:\Users\bou\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\bou\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\bou\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\bou\.minikube}
I0621 11:39:28.164264   16580 ubuntu.go:177] setting up certificates
I0621 11:39:28.164264   16580 provision.go:84] configureAuth start
I0621 11:39:28.168499   16580 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0621 11:39:28.247750   16580 provision.go:143] copyHostCerts
I0621 11:39:28.247750   16580 exec_runner.go:144] found C:\Users\bou\.minikube/cert.pem, removing ...
I0621 11:39:28.247750   16580 exec_runner.go:203] rm: C:\Users\bou\.minikube\cert.pem
I0621 11:39:28.247750   16580 exec_runner.go:151] cp: C:\Users\bou\.minikube\certs\cert.pem --> C:\Users\bou\.minikube/cert.pem (1115 bytes)
I0621 11:39:28.248270   16580 exec_runner.go:144] found C:\Users\bou\.minikube/key.pem, removing ...
I0621 11:39:28.248270   16580 exec_runner.go:203] rm: C:\Users\bou\.minikube\key.pem
I0621 11:39:28.248270   16580 exec_runner.go:151] cp: C:\Users\bou\.minikube\certs\key.pem --> C:\Users\bou\.minikube/key.pem (1679 bytes)
I0621 11:39:28.248786   16580 exec_runner.go:144] found C:\Users\bou\.minikube/ca.pem, removing ...
I0621 11:39:28.248786   16580 exec_runner.go:203] rm: C:\Users\bou\.minikube\ca.pem
I0621 11:39:28.248786   16580 exec_runner.go:151] cp: C:\Users\bou\.minikube\certs\ca.pem --> C:\Users\bou\.minikube/ca.pem (1070 bytes)
I0621 11:39:28.249304   16580 provision.go:117] generating server cert: C:\Users\bou\.minikube\machines\server.pem ca-key=C:\Users\bou\.minikube\certs\ca.pem private-key=C:\Users\bou\.minikube\certs\ca-key.pem org=bou.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0621 11:39:28.328541   16580 provision.go:177] copyRemoteCerts
I0621 11:39:28.334568   16580 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0621 11:39:28.338577   16580 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0621 11:39:28.418582   16580 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57276 SSHKeyPath:C:\Users\bou\.minikube\machines\minikube\id_rsa Username:docker}
I0621 11:39:28.508426   16580 ssh_runner.go:362] scp C:\Users\bou\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1070 bytes)
I0621 11:39:28.524502   16580 ssh_runner.go:362] scp C:\Users\bou\.minikube\machines\server.pem --> /etc/docker/server.pem (1172 bytes)
I0621 11:39:28.537927   16580 ssh_runner.go:362] scp C:\Users\bou\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0621 11:39:28.551497   16580 provision.go:87] duration metric: took 387.2339ms to configureAuth
I0621 11:39:28.551497   16580 ubuntu.go:193] setting minikube options for container-runtime
I0621 11:39:28.551497   16580 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0621 11:39:28.555303   16580 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0621 11:39:28.635122   16580 main.go:141] libmachine: Using SSH client type: native
I0621 11:39:28.635122   16580 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x5fa3c0] 0x5fcfa0 <nil>  [] 0s} 127.0.0.1 57276 <nil> <nil>}
I0621 11:39:28.635122   16580 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0621 11:39:28.756338   16580 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0621 11:39:28.756338   16580 ubuntu.go:71] root file system type: overlay
I0621 11:39:28.756338   16580 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0621 11:39:28.760046   16580 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0621 11:39:28.851723   16580 main.go:141] libmachine: Using SSH client type: native
I0621 11:39:28.851723   16580 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x5fa3c0] 0x5fcfa0 <nil>  [] 0s} 127.0.0.1 57276 <nil> <nil>}
I0621 11:39:28.851723   16580 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0621 11:39:28.981294   16580 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0621 11:39:28.985611   16580 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0621 11:39:29.067323   16580 main.go:141] libmachine: Using SSH client type: native
I0621 11:39:29.067323   16580 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x5fa3c0] 0x5fcfa0 <nil>  [] 0s} 127.0.0.1 57276 <nil> <nil>}
I0621 11:39:29.067323   16580 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0621 11:39:29.188816   16580 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0621 11:39:29.188816   16580 machine.go:97] duration metric: took 1.6928293s to provisionDockerMachine
I0621 11:39:29.188816   16580 start.go:293] postStartSetup for "minikube" (driver="docker")
I0621 11:39:29.188816   16580 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0621 11:39:29.195797   16580 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0621 11:39:29.199086   16580 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0621 11:39:29.280416   16580 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57276 SSHKeyPath:C:\Users\bou\.minikube\machines\minikube\id_rsa Username:docker}
I0621 11:39:29.384504   16580 ssh_runner.go:195] Run: cat /etc/os-release
I0621 11:39:29.386636   16580 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0621 11:39:29.386636   16580 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0621 11:39:29.386636   16580 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0621 11:39:29.386636   16580 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0621 11:39:29.386636   16580 filesync.go:126] Scanning C:\Users\bou\.minikube\addons for local assets ...
I0621 11:39:29.387154   16580 filesync.go:126] Scanning C:\Users\bou\.minikube\files for local assets ...
I0621 11:39:29.387154   16580 start.go:296] duration metric: took 198.3379ms for postStartSetup
I0621 11:39:29.393541   16580 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0621 11:39:29.397307   16580 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0621 11:39:29.480555   16580 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57276 SSHKeyPath:C:\Users\bou\.minikube\machines\minikube\id_rsa Username:docker}
I0621 11:39:29.572156   16580 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0621 11:39:29.575324   16580 fix.go:56] duration metric: took 2.759005s for fixHost
I0621 11:39:29.575324   16580 start.go:83] releasing machines lock for "minikube", held for 2.759005s
I0621 11:39:29.579558   16580 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0621 11:39:29.667011   16580 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0621 11:39:29.671344   16580 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0621 11:39:29.673483   16580 ssh_runner.go:195] Run: cat /version.json
I0621 11:39:29.677741   16580 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0621 11:39:29.757879   16580 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57276 SSHKeyPath:C:\Users\bou\.minikube\machines\minikube\id_rsa Username:docker}
I0621 11:39:29.773198   16580 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57276 SSHKeyPath:C:\Users\bou\.minikube\machines\minikube\id_rsa Username:docker}
I0621 11:39:29.851335   16580 ssh_runner.go:195] Run: systemctl --version
I0621 11:39:30.015555   16580 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0621 11:39:30.026067   16580 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0621 11:39:30.033482   16580 start.go:438] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0621 11:39:30.040442   16580 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0621 11:39:30.045712   16580 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0621 11:39:30.045712   16580 start.go:494] detecting cgroup driver to use...
I0621 11:39:30.045712   16580 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0621 11:39:30.046236   16580 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0621 11:39:30.062187   16580 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0621 11:39:30.074934   16580 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0621 11:39:30.081363   16580 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0621 11:39:30.087777   16580 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0621 11:39:30.100503   16580 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0621 11:39:30.112715   16580 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0621 11:39:30.125762   16580 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0621 11:39:30.138278   16580 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0621 11:39:30.151019   16580 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0621 11:39:30.163759   16580 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0621 11:39:30.176461   16580 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0621 11:39:30.190390   16580 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0621 11:39:30.202944   16580 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0621 11:39:30.214680   16580 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0621 11:39:30.314094   16580 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0621 11:39:30.395018   16580 start.go:494] detecting cgroup driver to use...
I0621 11:39:30.395018   16580 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0621 11:39:30.401924   16580 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0621 11:39:30.409211   16580 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0621 11:39:30.416167   16580 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0621 11:39:30.424346   16580 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0621 11:39:30.441136   16580 ssh_runner.go:195] Run: which cri-dockerd
I0621 11:39:30.450613   16580 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0621 11:39:30.459133   16580 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0621 11:39:30.479617   16580 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0621 11:39:30.578796   16580 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0621 11:39:30.666291   16580 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0621 11:39:30.666291   16580 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0621 11:39:30.683960   16580 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0621 11:39:30.774183   16580 ssh_runner.go:195] Run: sudo systemctl restart docker
I0621 11:39:31.028578   16580 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0621 11:39:31.042652   16580 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0621 11:39:31.057399   16580 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0621 11:39:31.070829   16580 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0621 11:39:31.154106   16580 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0621 11:39:31.243535   16580 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0621 11:39:31.332512   16580 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0621 11:39:31.347456   16580 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0621 11:39:31.361221   16580 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0621 11:39:31.413596   16580 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0621 11:39:31.468097   16580 start.go:541] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0621 11:39:31.474804   16580 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0621 11:39:31.477978   16580 start.go:562] Will wait 60s for crictl version
I0621 11:39:31.484450   16580 ssh_runner.go:195] Run: which crictl
I0621 11:39:31.493553   16580 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0621 11:39:31.625333   16580 start.go:578] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  26.1.1
RuntimeApiVersion:  v1
I0621 11:39:31.629532   16580 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0621 11:39:31.650794   16580 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0621 11:39:31.667027   16580 out.go:204] * Vorbereiten von Kubernetes v1.30.0 auf Docker 26.1.1...
I0621 11:39:31.671225   16580 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0621 11:39:31.814264   16580 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0621 11:39:31.820764   16580 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0621 11:39:31.824105   16580 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0621 11:39:31.834745   16580 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0621 11:39:31.921557   16580 kubeadm.go:877] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:7900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\bou:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0621 11:39:31.922061   16580 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0621 11:39:31.925848   16580 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0621 11:39:31.942702   16580 docker.go:685] Got preloaded images: -- stdout --
mohamed092/quarkus-app-jvm:latest
mohamed092/quarkus-app-jvm:<none>
mohamed092/quarkus-app-jvm:<none>
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0621 11:39:31.942702   16580 docker.go:615] Images already preloaded, skipping extraction
I0621 11:39:31.946378   16580 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0621 11:39:31.960536   16580 docker.go:685] Got preloaded images: -- stdout --
mohamed092/quarkus-app-jvm:latest
mohamed092/quarkus-app-jvm:<none>
mohamed092/quarkus-app-jvm:<none>
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0621 11:39:31.960536   16580 cache_images.go:84] Images are preloaded, skipping loading
I0621 11:39:31.960536   16580 kubeadm.go:928] updating node { 192.168.49.2 8443 v1.30.0 docker true true} ...
I0621 11:39:31.961050   16580 kubeadm.go:940] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.30.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0621 11:39:31.964761   16580 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0621 11:39:32.170523   16580 cni.go:84] Creating CNI manager for ""
I0621 11:39:32.170523   16580 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0621 11:39:32.170523   16580 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0621 11:39:32.170523   16580 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.30.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0621 11:39:32.170523   16580 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.30.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0621 11:39:32.177461   16580 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.30.0
I0621 11:39:32.184315   16580 binaries.go:44] Found k8s binaries, skipping transfer
I0621 11:39:32.190823   16580 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0621 11:39:32.196345   16580 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0621 11:39:32.206195   16580 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0621 11:39:32.216126   16580 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2150 bytes)
I0621 11:39:32.232723   16580 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0621 11:39:32.235824   16580 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0621 11:39:32.248680   16580 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0621 11:39:32.323881   16580 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0621 11:39:32.332403   16580 certs.go:68] Setting up C:\Users\bou\.minikube\profiles\minikube for IP: 192.168.49.2
I0621 11:39:32.332403   16580 certs.go:194] generating shared ca certs ...
I0621 11:39:32.332403   16580 certs.go:226] acquiring lock for ca certs: {Name:mk2a46d94b431d8ec88d6cd654bf1f1cd64d8596 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0621 11:39:32.332914   16580 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\bou\.minikube\ca.key
I0621 11:39:32.332914   16580 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\bou\.minikube\proxy-client-ca.key
I0621 11:39:32.332914   16580 certs.go:256] generating profile certs ...
I0621 11:39:32.333431   16580 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": C:\Users\bou\.minikube\profiles\minikube\client.key
I0621 11:39:32.333431   16580 certs.go:359] skipping valid signed profile cert regeneration for "minikube": C:\Users\bou\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I0621 11:39:32.333431   16580 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": C:\Users\bou\.minikube\profiles\minikube\proxy-client.key
I0621 11:39:32.333946   16580 certs.go:484] found cert: C:\Users\bou\.minikube\certs\ca-key.pem (1675 bytes)
I0621 11:39:32.333946   16580 certs.go:484] found cert: C:\Users\bou\.minikube\certs\ca.pem (1070 bytes)
I0621 11:39:32.333946   16580 certs.go:484] found cert: C:\Users\bou\.minikube\certs\cert.pem (1115 bytes)
I0621 11:39:32.333946   16580 certs.go:484] found cert: C:\Users\bou\.minikube\certs\key.pem (1679 bytes)
I0621 11:39:32.334462   16580 ssh_runner.go:362] scp C:\Users\bou\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0621 11:39:32.348962   16580 ssh_runner.go:362] scp C:\Users\bou\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0621 11:39:32.363550   16580 ssh_runner.go:362] scp C:\Users\bou\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0621 11:39:32.377583   16580 ssh_runner.go:362] scp C:\Users\bou\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0621 11:39:32.392715   16580 ssh_runner.go:362] scp C:\Users\bou\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0621 11:39:32.457191   16580 ssh_runner.go:362] scp C:\Users\bou\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0621 11:39:32.475603   16580 ssh_runner.go:362] scp C:\Users\bou\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0621 11:39:32.491219   16580 ssh_runner.go:362] scp C:\Users\bou\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0621 11:39:32.558041   16580 ssh_runner.go:362] scp C:\Users\bou\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0621 11:39:32.573672   16580 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (740 bytes)
I0621 11:39:32.592092   16580 ssh_runner.go:195] Run: openssl version
I0621 11:39:32.607279   16580 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0621 11:39:32.621496   16580 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0621 11:39:32.623570   16580 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Jun 20 08:17 /usr/share/ca-certificates/minikubeCA.pem
I0621 11:39:32.630372   16580 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0621 11:39:32.642463   16580 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0621 11:39:32.654592   16580 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0621 11:39:32.665119   16580 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0621 11:39:32.676566   16580 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0621 11:39:32.687907   16580 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0621 11:39:32.699573   16580 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0621 11:39:32.711071   16580 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0621 11:39:32.722797   16580 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0621 11:39:32.727584   16580 kubeadm.go:391] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:7900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\bou:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0621 11:39:32.731757   16580 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0621 11:39:32.751257   16580 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
W0621 11:39:32.757063   16580 kubeadm.go:404] apiserver tunnel failed: apiserver port not set
I0621 11:39:32.757063   16580 kubeadm.go:407] found existing configuration files, will attempt cluster restart
I0621 11:39:32.757063   16580 kubeadm.go:587] restartPrimaryControlPlane start ...
I0621 11:39:32.764071   16580 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0621 11:39:32.770496   16580 kubeadm.go:129] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0621 11:39:32.774178   16580 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0621 11:39:32.860450   16580 kubeconfig.go:47] verify endpoint returned: get endpoint: "minikube" does not appear in C:\Users\bou\.kube\config
I0621 11:39:32.860450   16580 kubeconfig.go:62] C:\Users\bou\.kube\config needs updating (will repair): [kubeconfig missing "minikube" cluster setting kubeconfig missing "minikube" context setting]
I0621 11:39:32.860968   16580 lock.go:35] WriteFile acquiring C:\Users\bou\.kube\config: {Name:mk9297b1d4c418e62a3df98461ce1b439c09093a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0621 11:39:32.875634   16580 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0621 11:39:32.882146   16580 kubeadm.go:624] The running cluster does not require reconfiguration: 127.0.0.1
I0621 11:39:32.882146   16580 kubeadm.go:591] duration metric: took 125.0835ms to restartPrimaryControlPlane
I0621 11:39:32.882146   16580 kubeadm.go:393] duration metric: took 154.5628ms to StartCluster
I0621 11:39:32.882146   16580 settings.go:142] acquiring lock: {Name:mk744b6c11de365096f9b5361be07ea4371bfd16 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0621 11:39:32.882146   16580 settings.go:150] Updating kubeconfig:  C:\Users\bou\.kube\config
I0621 11:39:32.882659   16580 lock.go:35] WriteFile acquiring C:\Users\bou\.kube\config: {Name:mk9297b1d4c418e62a3df98461ce1b439c09093a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0621 11:39:32.883222   16580 start.go:234] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0621 11:39:32.883222   16580 addons.go:502] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false yakd:false]
I0621 11:39:32.883743   16580 out.go:177] * Verifiziere Kubernetes Komponenten...
I0621 11:39:32.883222   16580 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0621 11:39:32.883743   16580 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0621 11:39:32.883743   16580 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0621 11:39:32.883743   16580 addons.go:234] Setting addon storage-provisioner=true in "minikube"
W0621 11:39:32.883743   16580 addons.go:243] addon storage-provisioner should already be in state true
I0621 11:39:32.883743   16580 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0621 11:39:32.884270   16580 host.go:66] Checking if "minikube" exists ...
I0621 11:39:32.895310   16580 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0621 11:39:32.896355   16580 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0621 11:39:32.896355   16580 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0621 11:39:32.985640   16580 out.go:177]   - Verwende Image gcr.io/k8s-minikube/storage-provisioner:v5
I0621 11:39:32.986639   16580 addons.go:426] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0621 11:39:32.986639   16580 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0621 11:39:32.990640   16580 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0621 11:39:33.000639   16580 addons.go:234] Setting addon default-storageclass=true in "minikube"
W0621 11:39:33.000639   16580 addons.go:243] addon default-storageclass should already be in state true
I0621 11:39:33.000639   16580 host.go:66] Checking if "minikube" exists ...
I0621 11:39:33.002639   16580 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0621 11:39:33.008645   16580 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0621 11:39:33.065640   16580 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0621 11:39:33.081640   16580 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57276 SSHKeyPath:C:\Users\bou\.minikube\machines\minikube\id_rsa Username:docker}
I0621 11:39:33.096641   16580 addons.go:426] installing /etc/kubernetes/addons/storageclass.yaml
I0621 11:39:33.096641   16580 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0621 11:39:33.100641   16580 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0621 11:39:33.158639   16580 api_server.go:52] waiting for apiserver process to appear ...
I0621 11:39:33.165640   16580 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0621 11:39:33.190639   16580 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57276 SSHKeyPath:C:\Users\bou\.minikube\machines\minikube\id_rsa Username:docker}
I0621 11:39:33.272036   16580 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0621 11:39:33.368665   16580 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0621 11:39:33.675706   16580 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0621 11:39:35.509811   16580 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (2.2377781s)
I0621 11:39:35.509811   16580 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (2.1411483s)
I0621 11:39:35.509811   16580 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (1.8341073s)
I0621 11:39:35.509811   16580 api_server.go:72] duration metric: took 2.6265922s to wait for apiserver process to appear ...
I0621 11:39:35.509811   16580 api_server.go:88] waiting for apiserver healthz status ...
I0621 11:39:35.509811   16580 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57275/healthz ...
I0621 11:39:35.515036   16580 api_server.go:279] https://127.0.0.1:57275/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0621 11:39:35.515036   16580 api_server.go:103] status: https://127.0.0.1:57275/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0621 11:39:35.518702   16580 out.go:177] * Addons aktiviert: storage-provisioner, default-storageclass
I0621 11:39:35.519221   16580 addons.go:505] duration metric: took 2.6360017s for enable addons: enabled=[storage-provisioner default-storageclass]
I0621 11:39:36.024376   16580 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57275/healthz ...
I0621 11:39:36.052320   16580 api_server.go:279] https://127.0.0.1:57275/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0621 11:39:36.052320   16580 api_server.go:103] status: https://127.0.0.1:57275/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0621 11:39:36.522122   16580 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57275/healthz ...
I0621 11:39:36.525948   16580 api_server.go:279] https://127.0.0.1:57275/healthz returned 200:
ok
I0621 11:39:36.530675   16580 api_server.go:141] control plane version: v1.30.0
I0621 11:39:36.530675   16580 api_server.go:131] duration metric: took 1.0208652s to wait for apiserver health ...
I0621 11:39:36.530675   16580 system_pods.go:43] waiting for kube-system pods to appear ...
I0621 11:39:36.534883   16580 system_pods.go:59] 7 kube-system pods found
I0621 11:39:36.534883   16580 system_pods.go:61] "coredns-7db6d8ff4d-5vxl2" [6b8540f4-bdb4-4203-b459-e545a3aa198d] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0621 11:39:36.534883   16580 system_pods.go:61] "etcd-minikube" [c2b9a6fe-c226-4c76-bb6f-a673540dd95c] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0621 11:39:36.534883   16580 system_pods.go:61] "kube-apiserver-minikube" [92146af1-30e7-42fc-967c-87c96dcce464] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0621 11:39:36.534883   16580 system_pods.go:61] "kube-controller-manager-minikube" [7957564a-95bb-4b6b-aff0-225e0e3f61c5] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0621 11:39:36.534883   16580 system_pods.go:61] "kube-proxy-4x486" [75f0480a-7903-4e11-9e6a-cd5fb8024b14] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0621 11:39:36.534883   16580 system_pods.go:61] "kube-scheduler-minikube" [88e76579-6b03-490b-9415-572d284a58f6] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0621 11:39:36.534883   16580 system_pods.go:61] "storage-provisioner" [4ffd4004-2374-4711-9038-6c0e678a2e86] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0621 11:39:36.534883   16580 system_pods.go:74] duration metric: took 4.2077ms to wait for pod list to return data ...
I0621 11:39:36.534883   16580 kubeadm.go:576] duration metric: took 3.6516651s to wait for: map[apiserver:true system_pods:true]
I0621 11:39:36.534883   16580 node_conditions.go:102] verifying NodePressure condition ...
I0621 11:39:36.537022   16580 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0621 11:39:36.537022   16580 node_conditions.go:123] node cpu capacity is 16
I0621 11:39:36.537022   16580 node_conditions.go:105] duration metric: took 2.1396ms to run NodePressure ...
I0621 11:39:36.537022   16580 start.go:240] waiting for startup goroutines ...
I0621 11:39:36.537022   16580 start.go:245] waiting for cluster config update ...
I0621 11:39:36.537022   16580 start.go:254] writing updated cluster config ...
I0621 11:39:36.544056   16580 ssh_runner.go:195] Run: rm -f paused
I0621 11:39:36.602126   16580 start.go:600] kubectl: 1.29.1, cluster: 1.30.0 (minor skew: 1)
I0621 11:39:36.603797   16580 out.go:177] * Fertig! kubectl ist jetzt f√ºr die standardm√§√üige (default) Verwendung des Clusters "minikube" und des Namespaces "default" konfiguriert


==> Docker <==
Jun 21 09:39:30 minikube dockerd[1016]: time="2024-06-21T09:39:30.841068171Z" level=info msg="Loading containers: start."
Jun 21 09:39:30 minikube dockerd[1016]: time="2024-06-21T09:39:30.963876913Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Jun 21 09:39:30 minikube dockerd[1016]: time="2024-06-21T09:39:30.985307340Z" level=info msg="Loading containers: done."
Jun 21 09:39:30 minikube dockerd[1016]: time="2024-06-21T09:39:30.996929454Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
Jun 21 09:39:30 minikube dockerd[1016]: time="2024-06-21T09:39:30.996954066Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
Jun 21 09:39:30 minikube dockerd[1016]: time="2024-06-21T09:39:30.996958514Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Jun 21 09:39:30 minikube dockerd[1016]: time="2024-06-21T09:39:30.996961413Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Jun 21 09:39:30 minikube dockerd[1016]: time="2024-06-21T09:39:30.996973948Z" level=info msg="Docker daemon" commit=ac2de55 containerd-snapshotter=false storage-driver=overlay2 version=26.1.1
Jun 21 09:39:30 minikube dockerd[1016]: time="2024-06-21T09:39:30.997032058Z" level=info msg="Daemon has completed initialization"
Jun 21 09:39:31 minikube dockerd[1016]: time="2024-06-21T09:39:31.020032978Z" level=info msg="API listen on /var/run/docker.sock"
Jun 21 09:39:31 minikube dockerd[1016]: time="2024-06-21T09:39:31.020052984Z" level=info msg="API listen on [::]:2376"
Jun 21 09:39:31 minikube systemd[1]: Started Docker Application Container Engine.
Jun 21 09:39:31 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Jun 21 09:39:31 minikube cri-dockerd[1280]: time="2024-06-21T09:39:31Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Jun 21 09:39:31 minikube cri-dockerd[1280]: time="2024-06-21T09:39:31Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Jun 21 09:39:31 minikube cri-dockerd[1280]: time="2024-06-21T09:39:31Z" level=info msg="Start docker client with request timeout 0s"
Jun 21 09:39:31 minikube cri-dockerd[1280]: time="2024-06-21T09:39:31Z" level=info msg="Hairpin mode is set to hairpin-veth"
Jun 21 09:39:31 minikube cri-dockerd[1280]: time="2024-06-21T09:39:31Z" level=info msg="Loaded network plugin cni"
Jun 21 09:39:31 minikube cri-dockerd[1280]: time="2024-06-21T09:39:31Z" level=info msg="Docker cri networking managed by network plugin cni"
Jun 21 09:39:31 minikube cri-dockerd[1280]: time="2024-06-21T09:39:31Z" level=info msg="Setting cgroupDriver cgroupfs"
Jun 21 09:39:31 minikube cri-dockerd[1280]: time="2024-06-21T09:39:31Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Jun 21 09:39:31 minikube cri-dockerd[1280]: time="2024-06-21T09:39:31Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Jun 21 09:39:31 minikube cri-dockerd[1280]: time="2024-06-21T09:39:31Z" level=info msg="Start cri-dockerd grpc backend"
Jun 21 09:39:31 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Jun 21 09:39:32 minikube cri-dockerd[1280]: time="2024-06-21T09:39:32Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-7db6d8ff4d-5vxl2_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"16fd138ca4e8c8b2d2b052f9901e5bab0f7de846eaf5333fca0c722b5101339b\""
Jun 21 09:39:32 minikube cri-dockerd[1280]: time="2024-06-21T09:39:32Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-7db6d8ff4d-5vxl2_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"24261870f465882cd82c6d275de1a5fe7563f89f585a2a88e902073e252c0ab5\""
Jun 21 09:39:32 minikube cri-dockerd[1280]: time="2024-06-21T09:39:32Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"quarkus-app-57bbdc7bfb-zbhkj_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"2a7888d32804a39299105da514035919bd960642af6115ec752ab0df77959c59\""
Jun 21 09:39:32 minikube cri-dockerd[1280]: time="2024-06-21T09:39:32Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"quarkus-app-57bbdc7bfb-554dr_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"bcf5b97f1b488dd4c60305bebdc0957509923744f785756d372e1e888ccd3bf6\""
Jun 21 09:39:32 minikube cri-dockerd[1280]: time="2024-06-21T09:39:32Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"4e66cbf8981bf4909ab8373de51dd923400d19c78dd040bc09a77e61469ccf25\". Proceed without further sandbox information."
Jun 21 09:39:32 minikube cri-dockerd[1280]: time="2024-06-21T09:39:32Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"a0ca319ee0b136949a423c409e99cc455877e087f72e4bd8651eeff8c96db9f0\". Proceed without further sandbox information."
Jun 21 09:39:32 minikube cri-dockerd[1280]: time="2024-06-21T09:39:32Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"b317135f6774b4b8fa71eed03542e08e04491b08240220d9722bc8cd7969b637\". Proceed without further sandbox information."
Jun 21 09:39:32 minikube cri-dockerd[1280]: time="2024-06-21T09:39:32Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"02835f6f10fe7120cf909c2c6616f92243e59901832bcedcb538aeb92d0f5abf\". Proceed without further sandbox information."
Jun 21 09:39:32 minikube cri-dockerd[1280]: time="2024-06-21T09:39:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/255d166b40823c804c2472b217018a39c0ddaaebb987410480242c6d84c5aea5/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 21 09:39:32 minikube cri-dockerd[1280]: time="2024-06-21T09:39:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8706f2cc8e9aff51ed85951fb8f7c75f11413d5e909d62d056404ae9e1d1c052/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 21 09:39:33 minikube cri-dockerd[1280]: time="2024-06-21T09:39:33Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/454c068680c988c19f4102dbd2564b3aac812705a4a6ecc688bd0fa2717bb8ca/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 21 09:39:33 minikube cri-dockerd[1280]: time="2024-06-21T09:39:33Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ca81acda72c0b8ba79ccd9bb76ec849dfb3b644536794debd399d5bed8b98f6d/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 21 09:39:33 minikube cri-dockerd[1280]: time="2024-06-21T09:39:33Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-7db6d8ff4d-5vxl2_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"16fd138ca4e8c8b2d2b052f9901e5bab0f7de846eaf5333fca0c722b5101339b\""
Jun 21 09:39:35 minikube cri-dockerd[1280]: time="2024-06-21T09:39:35Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Jun 21 09:39:35 minikube cri-dockerd[1280]: time="2024-06-21T09:39:35Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/014b967e4e2de515cb30740295fc5424ffa314007de14f9cfd43997bd0895336/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 21 09:39:35 minikube cri-dockerd[1280]: time="2024-06-21T09:39:35Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8cdd02b41bece0f8a58fd21836132d47b4061675ced5f7fc7b61e81bac120a28/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 21 09:39:35 minikube cri-dockerd[1280]: time="2024-06-21T09:39:35Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/eff37ef8abc1000099c2974adcad4ee2745a0ccf00cd4ccf2838c0c15711b7b0/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 21 09:39:35 minikube cri-dockerd[1280]: time="2024-06-21T09:39:35Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/235bdc0522ca77752f9ac001f104d323f403eb13e11c3da0016b5a639f956290/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 21 09:39:35 minikube cri-dockerd[1280]: time="2024-06-21T09:39:35Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8bd1b627de71af06663e33ce751755c033437894033e0ca405a2853ae1f6b0f0/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 21 09:39:37 minikube cri-dockerd[1280]: time="2024-06-21T09:39:37Z" level=info msg="Stop pulling image mohamed092/quarkus-app-jvm:latest: Status: Image is up to date for mohamed092/quarkus-app-jvm:latest"
Jun 21 09:39:38 minikube cri-dockerd[1280]: time="2024-06-21T09:39:38Z" level=info msg="Stop pulling image mohamed092/quarkus-app-jvm:latest: Status: Image is up to date for mohamed092/quarkus-app-jvm:latest"
Jun 21 09:39:46 minikube dockerd[1016]: time="2024-06-21T09:39:46.267177228Z" level=info msg="ignoring event" container=f18ad8d03c75d5da24b3541d13d8aa90268b06ebbccfeafe82a14c4ab11384e5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 21 09:40:10 minikube dockerd[1016]: 2024/06/21 09:40:10 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jun 21 09:40:10 minikube dockerd[1016]: 2024/06/21 09:40:10 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jun 21 09:40:10 minikube dockerd[1016]: 2024/06/21 09:40:10 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jun 21 09:40:10 minikube dockerd[1016]: 2024/06/21 09:40:10 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jun 21 09:40:10 minikube dockerd[1016]: 2024/06/21 09:40:10 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jun 21 09:40:10 minikube dockerd[1016]: 2024/06/21 09:40:10 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jun 21 09:40:10 minikube dockerd[1016]: 2024/06/21 09:40:10 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jun 21 09:40:10 minikube dockerd[1016]: 2024/06/21 09:40:10 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jun 21 09:40:10 minikube dockerd[1016]: 2024/06/21 09:40:10 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jun 21 09:40:10 minikube dockerd[1016]: 2024/06/21 09:40:10 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jun 21 09:40:10 minikube dockerd[1016]: 2024/06/21 09:40:10 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jun 21 09:40:10 minikube dockerd[1016]: 2024/06/21 09:40:10 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jun 21 09:40:10 minikube dockerd[1016]: 2024/06/21 09:40:10 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jun 21 09:40:10 minikube dockerd[1016]: 2024/06/21 09:40:10 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)


==> container status <==
CONTAINER           IMAGE                                                                                                CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
df79216e36ebe       6e38f40d628db                                                                                        3 minutes ago       Running             storage-provisioner       8                   014b967e4e2de       storage-provisioner
74c17c33fbec3       mohamed092/quarkus-app-jvm@sha256:5f1f578bbd6177ad6a754aa1cb6aa39a99df7a1d521422a0f31cf0f7f681e823   3 minutes ago       Running             quarkus-app               1                   235bdc0522ca7       quarkus-app-57bbdc7bfb-554dr
c45629e87e93d       mohamed092/quarkus-app-jvm@sha256:5f1f578bbd6177ad6a754aa1cb6aa39a99df7a1d521422a0f31cf0f7f681e823   3 minutes ago       Running             quarkus-app               1                   8bd1b627de71a       quarkus-app-57bbdc7bfb-zbhkj
5649e8d06014d       cbb01a7bd410d                                                                                        3 minutes ago       Running             coredns                   4                   eff37ef8abc10       coredns-7db6d8ff4d-5vxl2
5b155ace6108c       a0bf559e280cf                                                                                        3 minutes ago       Running             kube-proxy                4                   8cdd02b41bece       kube-proxy-4x486
f18ad8d03c75d       6e38f40d628db                                                                                        3 minutes ago       Exited              storage-provisioner       7                   014b967e4e2de       storage-provisioner
017903f6dde7d       c42f13656d0b2                                                                                        3 minutes ago       Running             kube-apiserver            4                   ca81acda72c0b       kube-apiserver-minikube
e5c4623f69df4       3861cfcd7c04c                                                                                        3 minutes ago       Running             etcd                      4                   454c068680c98       etcd-minikube
a3d6fcad73fdd       259c8277fcbbc                                                                                        3 minutes ago       Running             kube-scheduler            4                   8706f2cc8e9af       kube-scheduler-minikube
a63ff6de5a16e       c7aad43836fa5                                                                                        3 minutes ago       Running             kube-controller-manager   4                   255d166b40823       kube-controller-manager-minikube
ad2e9eb9e7287       mohamed092/quarkus-app-jvm@sha256:5f1f578bbd6177ad6a754aa1cb6aa39a99df7a1d521422a0f31cf0f7f681e823   47 minutes ago      Exited              quarkus-app               0                   bcf5b97f1b488       quarkus-app-57bbdc7bfb-554dr
d09826472d288       mohamed092/quarkus-app-jvm@sha256:5f1f578bbd6177ad6a754aa1cb6aa39a99df7a1d521422a0f31cf0f7f681e823   47 minutes ago      Exited              quarkus-app               0                   2a7888d32804a       quarkus-app-57bbdc7bfb-zbhkj
ed23e66682818       cbb01a7bd410d                                                                                        3 hours ago         Exited              coredns                   3                   16fd138ca4e8c       coredns-7db6d8ff4d-5vxl2
2e842d5509ac2       a0bf559e280cf                                                                                        3 hours ago         Exited              kube-proxy                3                   f1e6e6f3ec7e5       kube-proxy-4x486
ddf349a0c6600       c7aad43836fa5                                                                                        3 hours ago         Exited              kube-controller-manager   3                   93834d2a68f78       kube-controller-manager-minikube
3001ee20474ef       259c8277fcbbc                                                                                        3 hours ago         Exited              kube-scheduler            3                   4cfbce616c2eb       kube-scheduler-minikube
823d79acb8515       3861cfcd7c04c                                                                                        3 hours ago         Exited              etcd                      3                   beb2f4aacd268       etcd-minikube
290ad08e4188d       c42f13656d0b2                                                                                        3 hours ago         Exited              kube-apiserver            3                   6c86ca68fc51c       kube-apiserver-minikube


==> coredns [5649e8d06014] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:52762 - 35584 "HINFO IN 8989636987244183444.4469563806564507323. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.199504394s
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Trace[1595781664]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (21-Jun-2024 09:39:36.281) (total time: 10070ms):
Trace[1595781664]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10070ms (09:39:46.351)
Trace[1595781664]: [10.070388617s] [10.070388617s] END
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Trace[1986293634]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (21-Jun-2024 09:39:36.281) (total time: 10070ms):
Trace[1986293634]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10070ms (09:39:46.351)
Trace[1986293634]: [10.070432216s] [10.070432216s] END
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Trace[1097467082]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (21-Jun-2024 09:39:36.281) (total time: 10070ms):
Trace[1097467082]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10070ms (09:39:46.351)
Trace[1097467082]: [10.070425964s] [10.070425964s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": net/http: TLS handshake timeout


==> coredns [ed23e6668281] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:37922 - 59592 "HINFO IN 5147612242611374195.2499262923948360622. udp 57 false 512" NXDOMAIN qr,rd,ra 57 1.121841481s
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Trace[1020266887]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (21-Jun-2024 07:12:46.525) (total time: 10004ms):
Trace[1020266887]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10003ms (07:12:56.529)
Trace[1020266887]: [10.004458509s] [10.004458509s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Trace[252093252]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (21-Jun-2024 07:12:46.525) (total time: 10004ms):
Trace[252093252]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10003ms (07:12:56.529)
Trace[252093252]: [10.004549804s] [10.004549804s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Trace[2143456425]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (21-Jun-2024 07:12:46.525) (total time: 10004ms):
Trace[2143456425]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10003ms (07:12:56.529)
Trace[2143456425]: [10.004611045s] [10.004611045s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=5883c09216182566a63dff4c326a6fc9ed2982ff
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_06_20T10_18_04_0700
                    minikube.k8s.io/version=v1.33.1
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 20 Jun 2024 08:18:02 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Fri, 21 Jun 2024 09:43:19 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Fri, 21 Jun 2024 09:39:35 +0000   Thu, 20 Jun 2024 08:18:01 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Fri, 21 Jun 2024 09:39:35 +0000   Thu, 20 Jun 2024 08:18:01 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Fri, 21 Jun 2024 09:39:35 +0000   Thu, 20 Jun 2024 08:18:01 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Fri, 21 Jun 2024 09:39:35 +0000   Thu, 20 Jun 2024 08:18:02 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                16
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             15968604Ki
  pods:               110
Allocatable:
  cpu:                16
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             15968604Ki
  pods:               110
System Info:
  Machine ID:                 8d0276bb08084b288d44292954e3be3f
  System UUID:                8d0276bb08084b288d44292954e3be3f
  Boot ID:                    dbc65940-34aa-403a-91a3-3d19f88f343c
  Kernel Version:             5.15.153.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://26.1.1
  Kubelet Version:            v1.30.0
  Kube-Proxy Version:         v1.30.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (9 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     quarkus-app-57bbdc7bfb-554dr        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         47m
  default                     quarkus-app-57bbdc7bfb-zbhkj        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         47m
  kube-system                 coredns-7db6d8ff4d-5vxl2            100m (0%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (1%!)(MISSING)     25h
  kube-system                 etcd-minikube                       100m (0%!)(MISSING)     0 (0%!)(MISSING)      100Mi (0%!)(MISSING)       0 (0%!)(MISSING)         25h
  kube-system                 kube-apiserver-minikube             250m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         25h
  kube-system                 kube-controller-manager-minikube    200m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         25h
  kube-system                 kube-proxy-4x486                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         25h
  kube-system                 kube-scheduler-minikube             100m (0%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         25h
  kube-system                 storage-provisioner                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         25h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (4%!)(MISSING)   0 (0%!)(MISSING)
  memory             170Mi (1%!)(MISSING)  170Mi (1%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                    From             Message
  ----    ------                   ----                   ----             -------
  Normal  Starting                 3m51s                  kube-proxy       
  Normal  Starting                 3m56s                  kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  3m56s (x8 over 3m56s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    3m56s (x8 over 3m56s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     3m56s (x7 over 3m56s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  3m56s                  kubelet          Updated Node Allocatable limit across pods
  Normal  RegisteredNode           3m41s                  node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[  +0.008721] WSL (1) WARNING: /usr/share/zoneinfo/Europe/Berlin not found. Is the tzdata package installed?
[  +0.141806] Exception: 
[  +0.000004] Operation canceled @p9io.cpp:258 (AcceptAsync)

[  +0.013137] blk_update_request: I/O error, dev sdc, sector 0 op 0x1:(WRITE) flags 0x800 phys_seg 0 prio class 0
[  +0.000510] blk_update_request: I/O error, dev sdc, sector 0 op 0x1:(WRITE) flags 0x800 phys_seg 0 prio class 0
[  +0.000330] blk_update_request: I/O error, dev sdc, sector 0 op 0x1:(WRITE) flags 0x800 phys_seg 0 prio class 0
[  +0.000469] blk_update_request: I/O error, dev sdc, sector 0 op 0x1:(WRITE) flags 0x800 phys_seg 0 prio class 0
[  +0.000428] blk_update_request: I/O error, dev sdc, sector 1073479680 op 0x1:(WRITE) flags 0x800 phys_seg 1 prio class 0
[  +0.000364] blk_update_request: I/O error, dev sdc, sector 1073479680 op 0x1:(WRITE) flags 0x800 phys_seg 1 prio class 0
[  +0.000355] Buffer I/O error on dev sdc, logical block 134184960, lost sync page write
[  +0.000217] JBD2: Error -5 detected when updating journal superblock for sdc-8.
[  +0.000191] Aborting journal on device sdc-8.
[  +0.000133] blk_update_request: I/O error, dev sdc, sector 1073479680 op 0x1:(WRITE) flags 0x800 phys_seg 1 prio class 0
[  +0.000245] blk_update_request: I/O error, dev sdc, sector 1073479680 op 0x1:(WRITE) flags 0x800 phys_seg 1 prio class 0
[  +0.000247] Buffer I/O error on dev sdc, logical block 134184960, lost sync page write
[  +0.000190] JBD2: Error -5 detected when updating journal superblock for sdc-8.
[  +0.000192] EXT4-fs error (device sdc): ext4_put_super:1196: comm weston: Couldn't clean up the journal
[  +0.000270] blk_update_request: I/O error, dev sdc, sector 0 op 0x1:(WRITE) flags 0x3800 phys_seg 1 prio class 0
[  +0.000276] blk_update_request: I/O error, dev sdc, sector 0 op 0x1:(WRITE) flags 0x3800 phys_seg 1 prio class 0
[  +0.000268] Buffer I/O error on dev sdc, logical block 0, lost sync page write
[  +0.000210] EXT4-fs (sdc): I/O error while writing superblock
[  +0.000157] EXT4-fs (sdc): Remounting filesystem read-only
[  +1.124285] WSL (2) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.000573] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.000453] WSL (1) ERROR: ConfigMountFsTab:2589: Processing fstab with mount -a failed.
[  +0.001474] WSL (1) ERROR: ConfigApplyWindowsLibPath:2537: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000002]  failed 2
[  +0.002341] WSL (3) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.000593] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.019897] WSL (1) WARNING: /usr/share/zoneinfo/Europe/Berlin not found. Is the tzdata package installed?
[  +0.095661] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.006606] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000393] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000267] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000367] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.065573] WSL (1) ERROR: ConfigApplyWindowsLibPath:2537: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000003]  failed 2
[  +0.021078] WSL (1) WARNING: /usr/share/zoneinfo/Europe/Berlin not found. Is the tzdata package installed?
[  +0.039824] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.001101] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000371] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000475] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000435] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.264346] netlink: 'init': attribute type 4 has an invalid length.
[  +0.205006] kmem.limit_in_bytes is deprecated and will be removed. Please report your usecase to linux-mm@kvack.org if you depend on this functionality.
[  +4.779442] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.000913] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000333] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000286] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000290] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.124852] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.001038] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000415] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000323] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000364] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.082357] new mount options do not match the existing superblock, will be ignored
[  +0.117777] systemd-journald[25]: File /var/log/journal/59020a1388514a44a827ddc896b6ea62/system.journal corrupted or uncleanly shut down, renaming and replacing.


==> etcd [823d79acb851] <==
{"level":"info","ts":"2024-06-21T08:27:43.798088Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":46904,"took":"3.036301ms","hash":646213736,"current-db-size-bytes":2682880,"current-db-size":"2.7 MB","current-db-size-in-use-bytes":1421312,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-06-21T08:27:43.798129Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":646213736,"revision":46904,"compact-revision":46661}
{"level":"info","ts":"2024-06-21T08:32:43.801173Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":47144}
{"level":"info","ts":"2024-06-21T08:32:43.802979Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":47144,"took":"1.692068ms","hash":2075878617,"current-db-size-bytes":2682880,"current-db-size":"2.7 MB","current-db-size-in-use-bytes":1421312,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-06-21T08:32:43.803011Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2075878617,"revision":47144,"compact-revision":46904}
{"level":"info","ts":"2024-06-21T08:37:43.805142Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":47385}
{"level":"info","ts":"2024-06-21T08:37:43.806991Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":47385,"took":"1.751568ms","hash":2746112683,"current-db-size-bytes":2682880,"current-db-size":"2.7 MB","current-db-size-in-use-bytes":1425408,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-06-21T08:37:43.807018Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2746112683,"revision":47385,"compact-revision":47144}
{"level":"info","ts":"2024-06-21T08:42:43.801947Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":47624}
{"level":"info","ts":"2024-06-21T08:42:43.803669Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":47624,"took":"1.623683ms","hash":1765543805,"current-db-size-bytes":2682880,"current-db-size":"2.7 MB","current-db-size-in-use-bytes":1417216,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-06-21T08:42:43.803696Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1765543805,"revision":47624,"compact-revision":47385}
{"level":"info","ts":"2024-06-21T08:45:15.973539Z","caller":"etcdserver/server.go:1401","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":60006,"local-member-snapshot-index":50005,"local-member-snapshot-count":10000}
{"level":"info","ts":"2024-06-21T08:45:15.977467Z","caller":"etcdserver/server.go:2420","msg":"saved snapshot","snapshot-index":60006}
{"level":"info","ts":"2024-06-21T08:45:15.977532Z","caller":"etcdserver/server.go:2450","msg":"compacted Raft logs","compact-index":55006}
{"level":"info","ts":"2024-06-21T08:45:42.986224Z","caller":"fileutil/purge.go:96","msg":"purged","path":"/var/lib/minikube/etcd/member/snap/0000000000000002-0000000000002711.snap"}
{"level":"info","ts":"2024-06-21T08:47:43.797931Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":47865}
{"level":"info","ts":"2024-06-21T08:47:43.799754Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":47865,"took":"1.717846ms","hash":1716731606,"current-db-size-bytes":2682880,"current-db-size":"2.7 MB","current-db-size-in-use-bytes":1384448,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-06-21T08:47:43.799784Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1716731606,"revision":47865,"compact-revision":47624}
{"level":"info","ts":"2024-06-21T08:52:43.801235Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":48106}
{"level":"info","ts":"2024-06-21T08:52:43.803362Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":48106,"took":"1.997701ms","hash":2922081357,"current-db-size-bytes":2682880,"current-db-size":"2.7 MB","current-db-size-in-use-bytes":1380352,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-06-21T08:52:43.803397Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2922081357,"revision":48106,"compact-revision":47865}
{"level":"warn","ts":"2024-06-21T08:55:57.063864Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"103.784989ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-06-21T08:55:57.063942Z","caller":"traceutil/trace.go:171","msg":"trace[1581032244] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:48549; }","duration":"103.873135ms","start":"2024-06-21T08:55:56.96005Z","end":"2024-06-21T08:55:57.063923Z","steps":["trace[1581032244] 'range keys from in-memory index tree'  (duration: 103.740551ms)"],"step_count":1}
{"level":"info","ts":"2024-06-21T08:57:43.805643Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":48346}
{"level":"info","ts":"2024-06-21T08:57:43.807474Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":48346,"took":"1.722615ms","hash":2902976921,"current-db-size-bytes":2682880,"current-db-size":"2.7 MB","current-db-size-in-use-bytes":1568768,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-06-21T08:57:43.807507Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2902976921,"revision":48346,"compact-revision":48106}
{"level":"info","ts":"2024-06-21T09:02:43.810816Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":48656}
{"level":"info","ts":"2024-06-21T09:02:43.812886Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":48656,"took":"1.960086ms","hash":543711512,"current-db-size-bytes":2682880,"current-db-size":"2.7 MB","current-db-size-in-use-bytes":1642496,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-06-21T09:02:43.812915Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":543711512,"revision":48656,"compact-revision":48346}
{"level":"info","ts":"2024-06-21T09:07:43.81488Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":48895}
{"level":"info","ts":"2024-06-21T09:07:43.816756Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":48895,"took":"1.763356ms","hash":863783539,"current-db-size-bytes":2682880,"current-db-size":"2.7 MB","current-db-size-in-use-bytes":1454080,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2024-06-21T09:07:43.816785Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":863783539,"revision":48895,"compact-revision":48656}
{"level":"info","ts":"2024-06-21T09:12:43.8185Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":49136}
{"level":"info","ts":"2024-06-21T09:12:43.820882Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":49136,"took":"2.275569ms","hash":2250180154,"current-db-size-bytes":2682880,"current-db-size":"2.7 MB","current-db-size-in-use-bytes":1458176,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2024-06-21T09:12:43.820911Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2250180154,"revision":49136,"compact-revision":48895}
{"level":"info","ts":"2024-06-21T09:17:43.822783Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":49377}
{"level":"info","ts":"2024-06-21T09:17:43.824675Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":49377,"took":"1.786732ms","hash":784598953,"current-db-size-bytes":2682880,"current-db-size":"2.7 MB","current-db-size-in-use-bytes":1449984,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-06-21T09:17:43.824713Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":784598953,"revision":49377,"compact-revision":49136}
{"level":"info","ts":"2024-06-21T09:22:43.827743Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":49617}
{"level":"info","ts":"2024-06-21T09:22:43.829645Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":49617,"took":"1.795276ms","hash":2550431598,"current-db-size-bytes":2682880,"current-db-size":"2.7 MB","current-db-size-in-use-bytes":1441792,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-06-21T09:22:43.829674Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2550431598,"revision":49617,"compact-revision":49377}
{"level":"info","ts":"2024-06-21T09:27:43.831573Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":49858}
{"level":"info","ts":"2024-06-21T09:27:43.833462Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":49858,"took":"1.759819ms","hash":2287364009,"current-db-size-bytes":2682880,"current-db-size":"2.7 MB","current-db-size-in-use-bytes":1441792,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-06-21T09:27:43.833491Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2287364009,"revision":49858,"compact-revision":49617}
{"level":"info","ts":"2024-06-21T09:32:43.836395Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":50097}
{"level":"info","ts":"2024-06-21T09:32:43.838241Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":50097,"took":"1.729872ms","hash":1154540555,"current-db-size-bytes":2682880,"current-db-size":"2.7 MB","current-db-size-in-use-bytes":1449984,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-06-21T09:32:43.838275Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1154540555,"revision":50097,"compact-revision":49858}
{"level":"info","ts":"2024-06-21T09:37:43.841497Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":50338}
{"level":"info","ts":"2024-06-21T09:37:43.843453Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":50338,"took":"1.849601ms","hash":358028914,"current-db-size-bytes":2682880,"current-db-size":"2.7 MB","current-db-size-in-use-bytes":1429504,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-06-21T09:37:43.84348Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":358028914,"revision":50338,"compact-revision":50097}
{"level":"info","ts":"2024-06-21T09:38:58.105494Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2024-06-21T09:38:58.105562Z","caller":"embed/etcd.go:375","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2024-06-21T09:38:58.105654Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-06-21T09:38:58.105794Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-06-21T09:38:58.161643Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2024-06-21T09:38:58.161718Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2024-06-21T09:38:58.16177Z","caller":"etcdserver/server.go:1471","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-06-21T09:38:58.165955Z","caller":"embed/etcd.go:579","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-06-21T09:38:58.166115Z","caller":"embed/etcd.go:584","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-06-21T09:38:58.166142Z","caller":"embed/etcd.go:377","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> etcd [e5c4623f69df] <==
{"level":"warn","ts":"2024-06-21T09:39:33.277003Z","caller":"embed/config.go:679","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-06-21T09:39:33.277052Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2024-06-21T09:39:33.27709Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"warn","ts":"2024-06-21T09:39:33.2771Z","caller":"embed/config.go:679","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-06-21T09:39:33.277106Z","caller":"embed/etcd.go:127","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-06-21T09:39:33.277119Z","caller":"embed/etcd.go:494","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-06-21T09:39:33.278216Z","caller":"embed/etcd.go:135","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2024-06-21T09:39:33.278291Z","caller":"embed/etcd.go:308","msg":"starting an etcd server","etcd-version":"3.5.12","git-sha":"e7b3bb6cc","go-version":"go1.20.13","go-os":"linux","go-arch":"amd64","max-cpu-set":16,"max-cpu-available":16,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2024-06-21T09:39:33.279326Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"923.004¬µs"}
{"level":"info","ts":"2024-06-21T09:39:33.861206Z","caller":"etcdserver/server.go:511","msg":"recovered v2 store from snapshot","snapshot-index":60006,"snapshot-size":"8.2 kB"}
{"level":"info","ts":"2024-06-21T09:39:33.861251Z","caller":"etcdserver/server.go:524","msg":"recovered v3 backend from snapshot","backend-size-bytes":2682880,"backend-size":"2.7 MB","backend-size-in-use-bytes":1028096,"backend-size-in-use":"1.0 MB"}
{"level":"info","ts":"2024-06-21T09:39:34.084014Z","caller":"etcdserver/raft.go:530","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":63317}
{"level":"info","ts":"2024-06-21T09:39:34.084203Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2024-06-21T09:39:34.084238Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 5"}
{"level":"info","ts":"2024-06-21T09:39:34.084249Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [aec36adc501070cc], term: 5, commit: 63317, applied: 60006, lastindex: 63317, lastterm: 5]"}
{"level":"info","ts":"2024-06-21T09:39:34.084381Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-06-21T09:39:34.084419Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","recovered-remote-peer-id":"aec36adc501070cc","recovered-remote-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-06-21T09:39:34.084429Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2024-06-21T09:39:34.085573Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-06-21T09:39:34.086261Z","caller":"mvcc/kvstore.go:341","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":50338}
{"level":"info","ts":"2024-06-21T09:39:34.087121Z","caller":"mvcc/kvstore.go:407","msg":"kvstore restored","current-rev":50639}
{"level":"info","ts":"2024-06-21T09:39:34.088452Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-06-21T09:39:34.090247Z","caller":"etcdserver/corrupt.go:96","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2024-06-21T09:39:34.090424Z","caller":"etcdserver/corrupt.go:177","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-06-21T09:39:34.090455Z","caller":"etcdserver/server.go:851","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.12","cluster-id":"fa54960ea34d58be","cluster-version":"3.5"}
{"level":"info","ts":"2024-06-21T09:39:34.090546Z","caller":"etcdserver/server.go:744","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2024-06-21T09:39:34.090579Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-06-21T09:39:34.090602Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-06-21T09:39:34.090611Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-06-21T09:39:34.153217Z","caller":"embed/etcd.go:726","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-06-21T09:39:34.15333Z","caller":"embed/etcd.go:597","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-06-21T09:39:34.153349Z","caller":"embed/etcd.go:569","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-06-21T09:39:34.153406Z","caller":"embed/etcd.go:277","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-06-21T09:39:34.153431Z","caller":"embed/etcd.go:857","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-06-21T09:39:34.184699Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 5"}
{"level":"info","ts":"2024-06-21T09:39:34.184743Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 5"}
{"level":"info","ts":"2024-06-21T09:39:34.184765Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 5"}
{"level":"info","ts":"2024-06-21T09:39:34.184772Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 6"}
{"level":"info","ts":"2024-06-21T09:39:34.184776Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 6"}
{"level":"info","ts":"2024-06-21T09:39:34.184781Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 6"}
{"level":"info","ts":"2024-06-21T09:39:34.184785Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 6"}
{"level":"info","ts":"2024-06-21T09:39:34.187245Z","caller":"etcdserver/server.go:2068","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-06-21T09:39:34.187311Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-06-21T09:39:34.187328Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-06-21T09:39:34.187419Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-06-21T09:39:34.187443Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-06-21T09:39:34.188361Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-06-21T09:39:34.188361Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"warn","ts":"2024-06-21T09:39:39.560477Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"100.379715ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/default/minikube.17dafb88b725a7fb\" ","response":"range_response_count:1 size:660"}
{"level":"info","ts":"2024-06-21T09:39:39.56056Z","caller":"traceutil/trace.go:171","msg":"trace[1668227672] range","detail":"{range_begin:/registry/events/default/minikube.17dafb88b725a7fb; range_end:; response_count:1; response_revision:50673; }","duration":"100.492592ms","start":"2024-06-21T09:39:39.460055Z","end":"2024-06-21T09:39:39.560547Z","steps":["trace[1668227672] 'range keys from in-memory index tree'  (duration: 100.277306ms)"],"step_count":1}
{"level":"info","ts":"2024-06-21T09:39:40.551677Z","caller":"traceutil/trace.go:171","msg":"trace[1186425141] transaction","detail":"{read_only:false; response_revision:50699; number_of_response:1; }","duration":"100.470183ms","start":"2024-06-21T09:39:40.451192Z","end":"2024-06-21T09:39:40.551662Z","steps":["trace[1186425141] 'process raft request'  (duration: 100.369888ms)"],"step_count":1}
{"level":"info","ts":"2024-06-21T09:39:40.662556Z","caller":"traceutil/trace.go:171","msg":"trace[555490559] transaction","detail":"{read_only:false; response_revision:50701; number_of_response:1; }","duration":"101.483171ms","start":"2024-06-21T09:39:40.561057Z","end":"2024-06-21T09:39:40.66254Z","steps":["trace[555490559] 'process raft request'  (duration: 94.302093ms)"],"step_count":1}
{"level":"info","ts":"2024-06-21T09:39:40.952527Z","caller":"traceutil/trace.go:171","msg":"trace[1453911399] transaction","detail":"{read_only:false; response_revision:50704; number_of_response:1; }","duration":"191.461504ms","start":"2024-06-21T09:39:40.761044Z","end":"2024-06-21T09:39:40.952506Z","steps":["trace[1453911399] 'process raft request'  (duration: 98.539369ms)","trace[1453911399] 'compare'  (duration: 92.81061ms)"],"step_count":2}
{"level":"warn","ts":"2024-06-21T09:39:45.753429Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"101.895701ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128030006990357968 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/pods/kube-system/etcd-minikube\" mod_revision:50646 > success:<request_put:<key:\"/registry/pods/kube-system/etcd-minikube\" value_size:5603 >> failure:<request_range:<key:\"/registry/pods/kube-system/etcd-minikube\" > >>","response":"size:18"}
{"level":"info","ts":"2024-06-21T09:39:45.7535Z","caller":"traceutil/trace.go:171","msg":"trace[1770563660] transaction","detail":"{read_only:false; response_revision:50731; number_of_response:1; }","duration":"196.680195ms","start":"2024-06-21T09:39:45.556811Z","end":"2024-06-21T09:39:45.753491Z","steps":["trace[1770563660] 'process raft request'  (duration: 94.419774ms)","trace[1770563660] 'compare'  (duration: 101.817797ms)"],"step_count":2}


==> kernel <==
 09:43:28 up  2:36,  0 users,  load average: 0.09, 0.16, 0.12
Linux minikube 5.15.153.1-microsoft-standard-WSL2 #1 SMP Fri Mar 29 23:14:13 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [017903f6dde7] <==
W0621 09:39:34.793174       1 genericapiserver.go:733] Skipping API apps/v1beta1 because it has no resources.
I0621 09:39:34.794077       1 handler.go:286] Adding GroupVersion admissionregistration.k8s.io v1 to ResourceManager
W0621 09:39:34.794096       1 genericapiserver.go:733] Skipping API admissionregistration.k8s.io/v1beta1 because it has no resources.
W0621 09:39:34.794122       1 genericapiserver.go:733] Skipping API admissionregistration.k8s.io/v1alpha1 because it has no resources.
I0621 09:39:34.794416       1 handler.go:286] Adding GroupVersion events.k8s.io v1 to ResourceManager
W0621 09:39:34.794433       1 genericapiserver.go:733] Skipping API events.k8s.io/v1beta1 because it has no resources.
I0621 09:39:34.800695       1 handler.go:286] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
W0621 09:39:34.800714       1 genericapiserver.go:733] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I0621 09:39:35.058009       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0621 09:39:35.058083       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0621 09:39:35.058144       1 secure_serving.go:213] Serving securely on [::]:8443
I0621 09:39:35.058160       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0621 09:39:35.058153       1 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0621 09:39:35.058193       1 available_controller.go:423] Starting AvailableConditionController
I0621 09:39:35.058197       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0621 09:39:35.058201       1 apf_controller.go:374] Starting API Priority and Fairness config controller
I0621 09:39:35.058322       1 aggregator.go:163] waiting for initial CRD sync...
I0621 09:39:35.058349       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0621 09:39:35.058365       1 system_namespaces_controller.go:67] Starting system namespaces controller
I0621 09:39:35.058422       1 controller.go:78] Starting OpenAPI AggregationController
I0621 09:39:35.058531       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0621 09:39:35.058550       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0621 09:39:35.058657       1 customresource_discovery_controller.go:289] Starting DiscoveryController
I0621 09:39:35.058726       1 controller.go:87] Starting OpenAPI V3 controller
I0621 09:39:35.058797       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0621 09:39:35.058689       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0621 09:39:35.058814       1 crd_finalizer.go:266] Starting CRDFinalizer
I0621 09:39:35.058846       1 controller.go:116] Starting legacy_token_tracking_controller
I0621 09:39:35.058853       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0621 09:39:35.058856       1 establishing_controller.go:76] Starting EstablishingController
I0621 09:39:35.058698       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0621 09:39:35.058854       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0621 09:39:35.058911       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0621 09:39:35.058670       1 controller.go:139] Starting OpenAPI controller
I0621 09:39:35.058680       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0621 09:39:35.058943       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0621 09:39:35.058797       1 naming_controller.go:291] Starting NamingConditionController
I0621 09:39:35.058668       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0621 09:39:35.059019       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0621 09:39:35.059031       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0621 09:39:35.155306       1 shared_informer.go:320] Caches are synced for node_authorizer
I0621 09:39:35.158278       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0621 09:39:35.158296       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0621 09:39:35.158308       1 policy_source.go:224] refreshing policies
I0621 09:39:35.158409       1 apf_controller.go:379] Running API Priority and Fairness config worker
I0621 09:39:35.158420       1 apf_controller.go:382] Running API Priority and Fairness periodic rebalancing process
I0621 09:39:35.158922       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0621 09:39:35.159048       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0621 09:39:35.159091       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0621 09:39:35.159125       1 shared_informer.go:320] Caches are synced for configmaps
I0621 09:39:35.159154       1 aggregator.go:165] initial CRD sync complete...
I0621 09:39:35.159161       1 autoregister_controller.go:141] Starting autoregister controller
I0621 09:39:35.159165       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0621 09:39:35.159169       1 cache.go:39] Caches are synced for autoregister controller
I0621 09:39:35.163292       1 handler_discovery.go:447] Starting ResourceDiscoveryManager
E0621 09:39:35.164892       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I0621 09:39:35.166802       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0621 09:39:36.061967       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0621 09:39:47.558648       1 controller.go:615] quota admission added evaluator for: endpoints
I0621 09:39:47.558843       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io


==> kube-apiserver [290ad08e4188] <==
W0621 09:39:03.681532       1 logging.go:59] [core] [Channel #49 SubChannel #50] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:03.706612       1 logging.go:59] [core] [Channel #5 SubChannel #6] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:03.713223       1 logging.go:59] [core] [Channel #145 SubChannel #146] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:03.714401       1 logging.go:59] [core] [Channel #34 SubChannel #35] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:03.749592       1 logging.go:59] [core] [Channel #52 SubChannel #53] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:03.777996       1 logging.go:59] [core] [Channel #148 SubChannel #149] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:03.826822       1 logging.go:59] [core] [Channel #19 SubChannel #20] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:03.833299       1 logging.go:59] [core] [Channel #43 SubChannel #44] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:06.136741       1 logging.go:59] [core] [Channel #91 SubChannel #92] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:06.142505       1 logging.go:59] [core] [Channel #157 SubChannel #158] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:06.304629       1 logging.go:59] [core] [Channel #76 SubChannel #77] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:06.425225       1 logging.go:59] [core] [Channel #142 SubChannel #143] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:06.428822       1 logging.go:59] [core] [Channel #106 SubChannel #107] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:06.478422       1 logging.go:59] [core] [Channel #103 SubChannel #104] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:06.556938       1 logging.go:59] [core] [Channel #25 SubChannel #26] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:06.574707       1 logging.go:59] [core] [Channel #2 SubChannel #3] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:06.582868       1 logging.go:59] [core] [Channel #15 SubChannel #16] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:06.810122       1 logging.go:59] [core] [Channel #1 SubChannel #4] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:06.868118       1 logging.go:59] [core] [Channel #94 SubChannel #95] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:06.869687       1 logging.go:59] [core] [Channel #181 SubChannel #182] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:06.935886       1 logging.go:59] [core] [Channel #163 SubChannel #164] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:06.938320       1 logging.go:59] [core] [Channel #169 SubChannel #170] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:06.942891       1 logging.go:59] [core] [Channel #10 SubChannel #11] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:06.957737       1 logging.go:59] [core] [Channel #37 SubChannel #38] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:07.032957       1 logging.go:59] [core] [Channel #55 SubChannel #56] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:07.052382       1 logging.go:59] [core] [Channel #85 SubChannel #86] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:07.071432       1 logging.go:59] [core] [Channel #175 SubChannel #176] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:07.072792       1 logging.go:59] [core] [Channel #97 SubChannel #98] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:07.119794       1 logging.go:59] [core] [Channel #160 SubChannel #161] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:07.129982       1 logging.go:59] [core] [Channel #139 SubChannel #140] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:07.250299       1 logging.go:59] [core] [Channel #79 SubChannel #80] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:07.255844       1 logging.go:59] [core] [Channel #151 SubChannel #152] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:07.282405       1 logging.go:59] [core] [Channel #31 SubChannel #32] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:07.325822       1 logging.go:59] [core] [Channel #166 SubChannel #167] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:07.326961       1 logging.go:59] [core] [Channel #178 SubChannel #179] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:07.339245       1 logging.go:59] [core] [Channel #58 SubChannel #59] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:07.345154       1 logging.go:59] [core] [Channel #46 SubChannel #47] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:07.376390       1 logging.go:59] [core] [Channel #124 SubChannel #125] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:07.388648       1 logging.go:59] [core] [Channel #136 SubChannel #137] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:07.422373       1 logging.go:59] [core] [Channel #154 SubChannel #155] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:07.454552       1 logging.go:59] [core] [Channel #61 SubChannel #62] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:07.492140       1 logging.go:59] [core] [Channel #127 SubChannel #128] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:07.547694       1 logging.go:59] [core] [Channel #115 SubChannel #116] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:07.565721       1 logging.go:59] [core] [Channel #133 SubChannel #134] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:07.627253       1 logging.go:59] [core] [Channel #5 SubChannel #6] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:07.662773       1 logging.go:59] [core] [Channel #34 SubChannel #35] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:07.668871       1 logging.go:59] [core] [Channel #70 SubChannel #71] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:07.706582       1 logging.go:59] [core] [Channel #88 SubChannel #89] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:07.734081       1 logging.go:59] [core] [Channel #112 SubChannel #113] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:07.787737       1 logging.go:59] [core] [Channel #22 SubChannel #23] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:07.805341       1 logging.go:59] [core] [Channel #73 SubChannel #74] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:07.869654       1 logging.go:59] [core] [Channel #82 SubChannel #83] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:07.871939       1 logging.go:59] [core] [Channel #145 SubChannel #146] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:07.877758       1 logging.go:59] [core] [Channel #130 SubChannel #131] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:07.930139       1 logging.go:59] [core] [Channel #52 SubChannel #53] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:07.930139       1 logging.go:59] [core] [Channel #100 SubChannel #101] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:07.976126       1 logging.go:59] [core] [Channel #118 SubChannel #119] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:08.045354       1 logging.go:59] [core] [Channel #121 SubChannel #122] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:08.067565       1 logging.go:59] [core] [Channel #64 SubChannel #65] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 09:39:08.091571       1 logging.go:59] [core] [Channel #49 SubChannel #50] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-controller-manager [a63ff6de5a16] <==
I0621 09:39:47.368865       1 controllermanager.go:759] "Started controller" controller="cronjob-controller"
I0621 09:39:47.369017       1 cronjob_controllerv2.go:139] "Starting cronjob controller v2" logger="cronjob-controller"
I0621 09:39:47.369033       1 shared_informer.go:313] Waiting for caches to sync for cronjob
I0621 09:39:47.372105       1 shared_informer.go:313] Waiting for caches to sync for resource quota
I0621 09:39:47.453070       1 actual_state_of_world.go:543] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0621 09:39:47.460259       1 shared_informer.go:320] Caches are synced for node
I0621 09:39:47.460301       1 range_allocator.go:175] "Sending events to api server" logger="node-ipam-controller"
I0621 09:39:47.460316       1 range_allocator.go:179] "Starting range CIDR allocator" logger="node-ipam-controller"
I0621 09:39:47.460330       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0621 09:39:47.460334       1 shared_informer.go:320] Caches are synced for cidrallocator
I0621 09:39:47.461116       1 shared_informer.go:313] Waiting for caches to sync for garbage collector
I0621 09:39:47.461202       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I0621 09:39:47.461868       1 shared_informer.go:320] Caches are synced for PVC protection
I0621 09:39:47.462393       1 shared_informer.go:320] Caches are synced for namespace
I0621 09:39:47.463471       1 shared_informer.go:320] Caches are synced for ReplicationController
I0621 09:39:47.464768       1 shared_informer.go:320] Caches are synced for service account
I0621 09:39:47.465910       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I0621 09:39:47.467091       1 shared_informer.go:320] Caches are synced for daemon sets
I0621 09:39:47.467180       1 shared_informer.go:320] Caches are synced for TTL
I0621 09:39:47.469397       1 shared_informer.go:320] Caches are synced for cronjob
I0621 09:39:47.472685       1 shared_informer.go:320] Caches are synced for HPA
I0621 09:39:47.473412       1 shared_informer.go:320] Caches are synced for taint
I0621 09:39:47.473489       1 node_lifecycle_controller.go:1227] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0621 09:39:47.473544       1 node_lifecycle_controller.go:879] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0621 09:39:47.473576       1 node_lifecycle_controller.go:1073] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0621 09:39:47.475375       1 shared_informer.go:320] Caches are synced for TTL after finished
I0621 09:39:47.476543       1 shared_informer.go:320] Caches are synced for job
I0621 09:39:47.476584       1 shared_informer.go:320] Caches are synced for deployment
I0621 09:39:47.477796       1 shared_informer.go:320] Caches are synced for stateful set
I0621 09:39:47.478994       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I0621 09:39:47.551221       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0621 09:39:47.551221       1 shared_informer.go:320] Caches are synced for endpoint
I0621 09:39:47.551483       1 shared_informer.go:320] Caches are synced for PV protection
I0621 09:39:47.551539       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0621 09:39:47.551564       1 shared_informer.go:320] Caches are synced for persistent volume
I0621 09:39:47.551624       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0621 09:39:47.551794       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0621 09:39:47.551795       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/quarkus-app-57bbdc7bfb" duration="99.224¬µs"
I0621 09:39:47.551831       1 shared_informer.go:320] Caches are synced for ephemeral
I0621 09:39:47.551904       1 shared_informer.go:320] Caches are synced for GC
I0621 09:39:47.551900       1 shared_informer.go:320] Caches are synced for disruption
I0621 09:39:47.551937       1 shared_informer.go:320] Caches are synced for crt configmap
I0621 09:39:47.551995       1 shared_informer.go:320] Caches are synced for expand
I0621 09:39:47.552297       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/quarkus-app-554dc865bf" duration="647.699¬µs"
I0621 09:39:47.554252       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0621 09:39:47.554741       1 shared_informer.go:320] Caches are synced for attach detach
I0621 09:39:47.564745       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I0621 09:39:47.564784       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I0621 09:39:47.564817       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0621 09:39:47.564856       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I0621 09:39:47.566102       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="14.339664ms"
I0621 09:39:47.566199       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="34.858¬µs"
I0621 09:39:47.672801       1 shared_informer.go:320] Caches are synced for resource quota
I0621 09:39:47.751342       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I0621 09:39:47.751358       1 shared_informer.go:320] Caches are synced for resource quota
I0621 09:39:48.162087       1 shared_informer.go:320] Caches are synced for garbage collector
I0621 09:39:48.168741       1 shared_informer.go:320] Caches are synced for garbage collector
I0621 09:39:48.168765       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I0621 09:39:51.264453       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="4.916565ms"
I0621 09:39:51.264524       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="26.729¬µs"


==> kube-controller-manager [ddf349a0c660] <==
I0621 07:12:57.008946       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I0621 07:12:57.008966       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I0621 07:12:57.008973       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I0621 07:12:57.010070       1 shared_informer.go:320] Caches are synced for PV protection
I0621 07:12:57.011195       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0621 07:12:57.012405       1 shared_informer.go:320] Caches are synced for service account
I0621 07:12:57.013555       1 shared_informer.go:320] Caches are synced for TTL
I0621 07:12:57.015775       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0621 07:12:57.042308       1 shared_informer.go:320] Caches are synced for attach detach
I0621 07:12:57.098258       1 shared_informer.go:320] Caches are synced for persistent volume
I0621 07:12:57.177895       1 shared_informer.go:320] Caches are synced for disruption
I0621 07:12:57.207945       1 shared_informer.go:320] Caches are synced for resource quota
I0621 07:12:57.217282       1 shared_informer.go:320] Caches are synced for resource quota
I0621 07:12:57.626454       1 shared_informer.go:320] Caches are synced for garbage collector
I0621 07:12:57.638647       1 shared_informer.go:320] Caches are synced for garbage collector
I0621 07:12:57.638669       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I0621 07:12:58.377741       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="4.823829ms"
I0621 07:12:58.377804       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="29.623¬µs"
I0621 07:12:58.519511       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/quarkus-app-554dc865bf" duration="26.065¬µs"
I0621 07:12:59.518822       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/quarkus-app-554dc865bf" duration="61.322¬µs"
I0621 07:13:09.518783       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/quarkus-app-554dc865bf" duration="31.161¬µs"
I0621 07:13:14.520080       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/quarkus-app-554dc865bf" duration="39.635¬µs"
I0621 07:13:20.519451       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/quarkus-app-554dc865bf" duration="25.594¬µs"
I0621 07:13:27.518936       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/quarkus-app-554dc865bf" duration="29.297¬µs"
I0621 07:13:35.519078       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/quarkus-app-554dc865bf" duration="30.995¬µs"
I0621 07:13:39.519276       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/quarkus-app-554dc865bf" duration="28.158¬µs"
I0621 07:13:48.519828       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/quarkus-app-554dc865bf" duration="35.284¬µs"
I0621 07:14:16.519153       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/quarkus-app-554dc865bf" duration="32.855¬µs"
I0621 07:14:20.517992       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/quarkus-app-554dc865bf" duration="37.16¬µs"
I0621 07:14:28.535317       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/quarkus-app-554dc865bf" duration="39.458¬µs"
I0621 07:14:35.524007       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/quarkus-app-554dc865bf" duration="32.656¬µs"
I0621 07:15:19.628466       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/quarkus-app-554dc865bf" duration="4.100801ms"
I0621 07:15:19.628520       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/quarkus-app-554dc865bf" duration="20.368¬µs"
I0621 07:15:24.748192       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/quarkus-app-554dc865bf" duration="3.960018ms"
I0621 07:15:24.748263       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/quarkus-app-554dc865bf" duration="28.706¬µs"
I0621 08:55:52.661985       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/quarkus-app-57bbdc7bfb" duration="17.158121ms"
I0621 08:55:52.665483       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/quarkus-app-57bbdc7bfb" duration="3.447708ms"
I0621 08:55:52.665548       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/quarkus-app-57bbdc7bfb" duration="32.04¬µs"
I0621 08:55:52.672798       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/quarkus-app-57bbdc7bfb" duration="19.375¬µs"
I0621 08:55:54.618955       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/quarkus-app-57bbdc7bfb" duration="3.192394ms"
I0621 08:55:54.619024       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/quarkus-app-57bbdc7bfb" duration="23.33¬µs"
I0621 08:55:54.633076       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/quarkus-app-554dc865bf" duration="10.30643ms"
I0621 08:55:54.640048       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/quarkus-app-554dc865bf" duration="6.923387ms"
I0621 08:55:54.640103       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/quarkus-app-554dc865bf" duration="21.903¬µs"
I0621 08:55:54.659591       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/quarkus-app-57bbdc7bfb" duration="26.550988ms"
I0621 08:55:54.663826       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/quarkus-app-57bbdc7bfb" duration="4.202637ms"
I0621 08:55:54.663894       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/quarkus-app-57bbdc7bfb" duration="23.187¬µs"
I0621 08:55:54.664375       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/quarkus-app-57bbdc7bfb" duration="22.373¬µs"
I0621 08:55:55.466289       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/quarkus-app-554dc865bf" duration="40.997¬µs"
I0621 08:55:55.633115       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/quarkus-app-554dc865bf" duration="37.211¬µs"
I0621 08:55:55.656448       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/quarkus-app-554dc865bf" duration="30.297¬µs"
I0621 08:55:55.656485       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/quarkus-app-554dc865bf" duration="10.797¬µs"
I0621 08:55:57.966022       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/quarkus-app-57bbdc7bfb" duration="5.789069ms"
I0621 08:55:57.966091       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/quarkus-app-57bbdc7bfb" duration="32.377¬µs"
I0621 08:55:58.159461       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/quarkus-app-554dc865bf" duration="101.850487ms"
I0621 08:55:58.165273       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/quarkus-app-554dc865bf" duration="5.777394ms"
I0621 08:55:58.165349       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/quarkus-app-554dc865bf" duration="29.859¬µs"
I0621 08:55:59.163418       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/quarkus-app-554dc865bf" duration="34.711¬µs"
I0621 08:56:00.162270       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/quarkus-app-554dc865bf" duration="46.169¬µs"
I0621 08:56:00.165689       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/quarkus-app-554dc865bf" duration="22.28¬µs"


==> kube-proxy [2e842d5509ac] <==
I0621 07:12:46.403972       1 server_linux.go:69] "Using iptables proxy"
I0621 07:12:46.425849       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
I0621 07:12:46.461007       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0621 07:12:46.461046       1 server_linux.go:165] "Using iptables Proxier"
I0621 07:12:46.462077       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0621 07:12:46.462097       1 server_linux.go:528] "Defaulting to no-op detect-local"
I0621 07:12:46.462107       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0621 07:12:46.462792       1 server.go:872] "Version info" version="v1.30.0"
I0621 07:12:46.462822       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0621 07:12:46.465107       1 config.go:192] "Starting service config controller"
I0621 07:12:46.465439       1 config.go:101] "Starting endpoint slice config controller"
I0621 07:12:46.465489       1 shared_informer.go:313] Waiting for caches to sync for service config
I0621 07:12:46.465517       1 config.go:319] "Starting node config controller"
I0621 07:12:46.465547       1 shared_informer.go:313] Waiting for caches to sync for node config
I0621 07:12:46.465497       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0621 07:12:46.565721       1 shared_informer.go:320] Caches are synced for node config
I0621 07:12:46.565751       1 shared_informer.go:320] Caches are synced for service config
I0621 07:12:46.565755       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-proxy [5b155ace6108] <==
I0621 09:39:36.182821       1 server_linux.go:69] "Using iptables proxy"
I0621 09:39:36.253667       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
I0621 09:39:36.273222       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0621 09:39:36.273287       1 server_linux.go:165] "Using iptables Proxier"
I0621 09:39:36.274518       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0621 09:39:36.274542       1 server_linux.go:528] "Defaulting to no-op detect-local"
I0621 09:39:36.274556       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0621 09:39:36.274716       1 server.go:872] "Version info" version="v1.30.0"
I0621 09:39:36.274742       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0621 09:39:36.275434       1 config.go:192] "Starting service config controller"
I0621 09:39:36.275457       1 shared_informer.go:313] Waiting for caches to sync for service config
I0621 09:39:36.275529       1 config.go:319] "Starting node config controller"
I0621 09:39:36.275537       1 shared_informer.go:313] Waiting for caches to sync for node config
I0621 09:39:36.275550       1 config.go:101] "Starting endpoint slice config controller"
I0621 09:39:36.275555       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0621 09:39:36.376384       1 shared_informer.go:320] Caches are synced for service config
I0621 09:39:36.376403       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0621 09:39:36.376409       1 shared_informer.go:320] Caches are synced for node config


==> kube-scheduler [3001ee20474e] <==
I0621 07:12:42.937233       1 serving.go:380] Generated self-signed cert in-memory
I0621 07:12:44.520546       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.30.0"
I0621 07:12:44.520583       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0621 07:12:44.599326       1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0621 07:12:44.599355       1 shared_informer.go:313] Waiting for caches to sync for RequestHeaderAuthRequestController
I0621 07:12:44.599349       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0621 07:12:44.599559       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0621 07:12:44.599335       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0621 07:12:44.600104       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0621 07:12:44.600135       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0621 07:12:44.600154       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0621 07:12:44.700143       1 shared_informer.go:320] Caches are synced for RequestHeaderAuthRequestController
I0621 07:12:44.700168       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0621 07:12:44.700484       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0621 09:38:58.152547       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
I0621 09:38:58.152631       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
E0621 09:38:58.152631       1 run.go:74] "command failed" err="finished without leader elect"


==> kube-scheduler [a3d6fcad73fd] <==
I0621 09:39:33.956182       1 serving.go:380] Generated self-signed cert in-memory
W0621 09:39:35.065144       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0621 09:39:35.065166       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0621 09:39:35.065174       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0621 09:39:35.065181       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0621 09:39:35.156738       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.30.0"
I0621 09:39:35.156761       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0621 09:39:35.157989       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0621 09:39:35.158029       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0621 09:39:35.158062       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0621 09:39:35.158466       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0621 09:39:35.258981       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Jun 21 09:39:32 minikube kubelet[1512]: I0621 09:39:32.584535    1512 scope.go:117] "RemoveContainer" containerID="a1bf632652ee5a4a3b6810d0e666a248cc336760b577872220dca831a752362c"
Jun 21 09:39:32 minikube kubelet[1512]: E0621 09:39:32.584997    1512 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: a1bf632652ee5a4a3b6810d0e666a248cc336760b577872220dca831a752362c" containerID="a1bf632652ee5a4a3b6810d0e666a248cc336760b577872220dca831a752362c"
Jun 21 09:39:32 minikube kubelet[1512]: I0621 09:39:32.585026    1512 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"a1bf632652ee5a4a3b6810d0e666a248cc336760b577872220dca831a752362c"} err="failed to get container status \"a1bf632652ee5a4a3b6810d0e666a248cc336760b577872220dca831a752362c\": rpc error: code = Unknown desc = Error response from daemon: No such container: a1bf632652ee5a4a3b6810d0e666a248cc336760b577872220dca831a752362c"
Jun 21 09:39:32 minikube kubelet[1512]: I0621 09:39:32.585039    1512 scope.go:117] "RemoveContainer" containerID="a1bf632652ee5a4a3b6810d0e666a248cc336760b577872220dca831a752362c"
Jun 21 09:39:32 minikube kubelet[1512]: I0621 09:39:32.585640    1512 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"a1bf632652ee5a4a3b6810d0e666a248cc336760b577872220dca831a752362c"} err="failed to get container status \"a1bf632652ee5a4a3b6810d0e666a248cc336760b577872220dca831a752362c\": rpc error: code = Unknown desc = Error response from daemon: No such container: a1bf632652ee5a4a3b6810d0e666a248cc336760b577872220dca831a752362c"
Jun 21 09:39:32 minikube kubelet[1512]: E0621 09:39:32.590391    1512 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused" interval="400ms"
Jun 21 09:39:32 minikube kubelet[1512]: I0621 09:39:32.591490    1512 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/063d6b9688927e601f52fd818d1305c5-etcd-data\") pod \"etcd-minikube\" (UID: \"063d6b9688927e601f52fd818d1305c5\") " pod="kube-system/etcd-minikube"
Jun 21 09:39:32 minikube kubelet[1512]: I0621 09:39:32.591519    1512 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/3c555f828409b009ebee39fdbedfcac0-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"3c555f828409b009ebee39fdbedfcac0\") " pod="kube-system/kube-apiserver-minikube"
Jun 21 09:39:32 minikube kubelet[1512]: I0621 09:39:32.591533    1512 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/3c555f828409b009ebee39fdbedfcac0-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"3c555f828409b009ebee39fdbedfcac0\") " pod="kube-system/kube-apiserver-minikube"
Jun 21 09:39:32 minikube kubelet[1512]: I0621 09:39:32.591543    1512 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/7fd44e8d11c3e0ffe6b1825e2a1f2270-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"7fd44e8d11c3e0ffe6b1825e2a1f2270\") " pod="kube-system/kube-controller-manager-minikube"
Jun 21 09:39:32 minikube kubelet[1512]: I0621 09:39:32.591552    1512 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/3c555f828409b009ebee39fdbedfcac0-usr-local-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"3c555f828409b009ebee39fdbedfcac0\") " pod="kube-system/kube-apiserver-minikube"
Jun 21 09:39:32 minikube kubelet[1512]: I0621 09:39:32.591562    1512 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/7fd44e8d11c3e0ffe6b1825e2a1f2270-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"7fd44e8d11c3e0ffe6b1825e2a1f2270\") " pod="kube-system/kube-controller-manager-minikube"
Jun 21 09:39:32 minikube kubelet[1512]: I0621 09:39:32.591569    1512 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/7fd44e8d11c3e0ffe6b1825e2a1f2270-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"7fd44e8d11c3e0ffe6b1825e2a1f2270\") " pod="kube-system/kube-controller-manager-minikube"
Jun 21 09:39:32 minikube kubelet[1512]: I0621 09:39:32.591579    1512 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/3c555f828409b009ebee39fdbedfcac0-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"3c555f828409b009ebee39fdbedfcac0\") " pod="kube-system/kube-apiserver-minikube"
Jun 21 09:39:32 minikube kubelet[1512]: I0621 09:39:32.591587    1512 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/3c555f828409b009ebee39fdbedfcac0-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"3c555f828409b009ebee39fdbedfcac0\") " pod="kube-system/kube-apiserver-minikube"
Jun 21 09:39:32 minikube kubelet[1512]: I0621 09:39:32.591607    1512 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/7fd44e8d11c3e0ffe6b1825e2a1f2270-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"7fd44e8d11c3e0ffe6b1825e2a1f2270\") " pod="kube-system/kube-controller-manager-minikube"
Jun 21 09:39:32 minikube kubelet[1512]: I0621 09:39:32.591647    1512 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/f9c8e1d0d74b1727abdb4b4a31d3a7c1-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"f9c8e1d0d74b1727abdb4b4a31d3a7c1\") " pod="kube-system/kube-scheduler-minikube"
Jun 21 09:39:32 minikube kubelet[1512]: I0621 09:39:32.591666    1512 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/063d6b9688927e601f52fd818d1305c5-etcd-certs\") pod \"etcd-minikube\" (UID: \"063d6b9688927e601f52fd818d1305c5\") " pod="kube-system/etcd-minikube"
Jun 21 09:39:32 minikube kubelet[1512]: I0621 09:39:32.591674    1512 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/7fd44e8d11c3e0ffe6b1825e2a1f2270-ca-certs\") pod \"kube-controller-manager-minikube\" (UID: \"7fd44e8d11c3e0ffe6b1825e2a1f2270\") " pod="kube-system/kube-controller-manager-minikube"
Jun 21 09:39:32 minikube kubelet[1512]: I0621 09:39:32.591682    1512 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/7fd44e8d11c3e0ffe6b1825e2a1f2270-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"7fd44e8d11c3e0ffe6b1825e2a1f2270\") " pod="kube-system/kube-controller-manager-minikube"
Jun 21 09:39:32 minikube kubelet[1512]: I0621 09:39:32.591690    1512 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/7fd44e8d11c3e0ffe6b1825e2a1f2270-usr-local-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"7fd44e8d11c3e0ffe6b1825e2a1f2270\") " pod="kube-system/kube-controller-manager-minikube"
Jun 21 09:39:32 minikube kubelet[1512]: I0621 09:39:32.691876    1512 kubelet_node_status.go:73] "Attempting to register node" node="minikube"
Jun 21 09:39:32 minikube kubelet[1512]: E0621 09:39:32.692151    1512 kubelet_node_status.go:96] "Unable to register node with API server" err="Post \"https://control-plane.minikube.internal:8443/api/v1/nodes\": dial tcp 192.168.49.2:8443: connect: connection refused" node="minikube"
Jun 21 09:39:32 minikube kubelet[1512]: E0621 09:39:32.990965    1512 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused" interval="800ms"
Jun 21 09:39:33 minikube kubelet[1512]: I0621 09:39:33.093652    1512 kubelet_node_status.go:73] "Attempting to register node" node="minikube"
Jun 21 09:39:33 minikube kubelet[1512]: E0621 09:39:33.094019    1512 kubelet_node_status.go:96] "Unable to register node with API server" err="Post \"https://control-plane.minikube.internal:8443/api/v1/nodes\": dial tcp 192.168.49.2:8443: connect: connection refused" node="minikube"
Jun 21 09:39:33 minikube kubelet[1512]: W0621 09:39:33.251544    1512 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.RuntimeClass: Get "https://control-plane.minikube.internal:8443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Jun 21 09:39:33 minikube kubelet[1512]: E0621 09:39:33.251614    1512 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: Get "https://control-plane.minikube.internal:8443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Jun 21 09:39:33 minikube kubelet[1512]: I0621 09:39:33.951824    1512 kubelet_node_status.go:73] "Attempting to register node" node="minikube"
Jun 21 09:39:35 minikube kubelet[1512]: I0621 09:39:35.169241    1512 kubelet_node_status.go:112] "Node was previously registered" node="minikube"
Jun 21 09:39:35 minikube kubelet[1512]: I0621 09:39:35.169308    1512 kubelet_node_status.go:76] "Successfully registered node" node="minikube"
Jun 21 09:39:35 minikube kubelet[1512]: I0621 09:39:35.170024    1512 kuberuntime_manager.go:1523] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Jun 21 09:39:35 minikube kubelet[1512]: I0621 09:39:35.170750    1512 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Jun 21 09:39:35 minikube kubelet[1512]: I0621 09:39:35.388104    1512 apiserver.go:52] "Watching apiserver"
Jun 21 09:39:35 minikube kubelet[1512]: I0621 09:39:35.389876    1512 topology_manager.go:215] "Topology Admit Handler" podUID="4ffd4004-2374-4711-9038-6c0e678a2e86" podNamespace="kube-system" podName="storage-provisioner"
Jun 21 09:39:35 minikube kubelet[1512]: I0621 09:39:35.390044    1512 topology_manager.go:215] "Topology Admit Handler" podUID="75f0480a-7903-4e11-9e6a-cd5fb8024b14" podNamespace="kube-system" podName="kube-proxy-4x486"
Jun 21 09:39:35 minikube kubelet[1512]: I0621 09:39:35.390118    1512 topology_manager.go:215] "Topology Admit Handler" podUID="6b8540f4-bdb4-4203-b459-e545a3aa198d" podNamespace="kube-system" podName="coredns-7db6d8ff4d-5vxl2"
Jun 21 09:39:35 minikube kubelet[1512]: I0621 09:39:35.390176    1512 topology_manager.go:215] "Topology Admit Handler" podUID="86eba7d5-0e87-4ba1-9d5e-5d96874a0141" podNamespace="default" podName="quarkus-app-57bbdc7bfb-zbhkj"
Jun 21 09:39:35 minikube kubelet[1512]: I0621 09:39:35.390219    1512 topology_manager.go:215] "Topology Admit Handler" podUID="36b6d025-dd89-4275-9ff8-8f54a7ccd9dc" podNamespace="default" podName="quarkus-app-57bbdc7bfb-554dr"
Jun 21 09:39:35 minikube kubelet[1512]: I0621 09:39:35.490467    1512 desired_state_of_world_populator.go:157] "Finished populating initial desired state of world"
Jun 21 09:39:35 minikube kubelet[1512]: I0621 09:39:35.569836    1512 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/75f0480a-7903-4e11-9e6a-cd5fb8024b14-lib-modules\") pod \"kube-proxy-4x486\" (UID: \"75f0480a-7903-4e11-9e6a-cd5fb8024b14\") " pod="kube-system/kube-proxy-4x486"
Jun 21 09:39:35 minikube kubelet[1512]: I0621 09:39:35.569927    1512 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/4ffd4004-2374-4711-9038-6c0e678a2e86-tmp\") pod \"storage-provisioner\" (UID: \"4ffd4004-2374-4711-9038-6c0e678a2e86\") " pod="kube-system/storage-provisioner"
Jun 21 09:39:35 minikube kubelet[1512]: I0621 09:39:35.569938    1512 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/75f0480a-7903-4e11-9e6a-cd5fb8024b14-xtables-lock\") pod \"kube-proxy-4x486\" (UID: \"75f0480a-7903-4e11-9e6a-cd5fb8024b14\") " pod="kube-system/kube-proxy-4x486"
Jun 21 09:39:35 minikube kubelet[1512]: E0621 09:39:35.679105    1512 kubelet.go:1928] "Failed creating a mirror pod for" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Jun 21 09:39:35 minikube kubelet[1512]: E0621 09:39:35.679282    1512 kubelet.go:1928] "Failed creating a mirror pod for" err="pods \"kube-scheduler-minikube\" already exists" pod="kube-system/kube-scheduler-minikube"
Jun 21 09:39:41 minikube kubelet[1512]: I0621 09:39:41.251273    1512 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Jun 21 09:39:42 minikube kubelet[1512]: E0621 09:39:42.657720    1512 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Jun 21 09:39:42 minikube kubelet[1512]: E0621 09:39:42.657780    1512 helpers.go:857] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
Jun 21 09:39:47 minikube kubelet[1512]: I0621 09:39:47.358495    1512 scope.go:117] "RemoveContainer" containerID="d599f202dc0d019519b3996951a078f4fcab2cd7b6ed7c8d5fbfe7ae461985f2"
Jun 21 09:39:47 minikube kubelet[1512]: I0621 09:39:47.358687    1512 scope.go:117] "RemoveContainer" containerID="f18ad8d03c75d5da24b3541d13d8aa90268b06ebbccfeafe82a14c4ab11384e5"
Jun 21 09:39:47 minikube kubelet[1512]: E0621 09:39:47.358830    1512 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(4ffd4004-2374-4711-9038-6c0e678a2e86)\"" pod="kube-system/storage-provisioner" podUID="4ffd4004-2374-4711-9038-6c0e678a2e86"
Jun 21 09:39:52 minikube kubelet[1512]: E0621 09:39:52.671734    1512 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Jun 21 09:39:52 minikube kubelet[1512]: E0621 09:39:52.671767    1512 helpers.go:857] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
Jun 21 09:39:59 minikube kubelet[1512]: I0621 09:39:59.400868    1512 scope.go:117] "RemoveContainer" containerID="f18ad8d03c75d5da24b3541d13d8aa90268b06ebbccfeafe82a14c4ab11384e5"
Jun 21 09:40:02 minikube kubelet[1512]: E0621 09:40:02.688807    1512 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Jun 21 09:40:02 minikube kubelet[1512]: E0621 09:40:02.688836    1512 helpers.go:857] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
Jun 21 09:40:12 minikube kubelet[1512]: E0621 09:40:12.702569    1512 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Jun 21 09:40:12 minikube kubelet[1512]: E0621 09:40:12.702599    1512 helpers.go:857] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
Jun 21 09:40:22 minikube kubelet[1512]: E0621 09:40:22.714966    1512 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Jun 21 09:40:22 minikube kubelet[1512]: E0621 09:40:22.715005    1512 helpers.go:857] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"


==> storage-provisioner [df79216e36eb] <==
I0621 09:39:59.470575       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0621 09:39:59.475655       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0621 09:39:59.475693       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0621 09:40:16.865712       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0621 09:40:16.865809       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_e50be0ef-f337-495c-a4c5-8d2d9567d973!
I0621 09:40:16.865957       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"4d631e1e-fa20-4412-b62e-009a95bf5dab", APIVersion:"v1", ResourceVersion:"50764", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_e50be0ef-f337-495c-a4c5-8d2d9567d973 became leader
I0621 09:40:16.965952       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_e50be0ef-f337-495c-a4c5-8d2d9567d973!


==> storage-provisioner [f18ad8d03c75] <==
I0621 09:39:36.154937       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0621 09:39:46.160480       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": net/http: TLS handshake timeout

